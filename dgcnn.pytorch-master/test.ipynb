{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import open3d as o3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.init as init\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from description file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print grid in readable format\n",
    "def pretty_print(grid):\n",
    "    for row in grid:\n",
    "        print(row)\n",
    "\n",
    "#from path of description file get the coupling grid, model data and \n",
    "#remove models with error\n",
    "def process_file(path):\n",
    "    with open(path,'r') as f:\n",
    "        #don't consider first two lines\n",
    "        f.readline()           #segmenti x x x\n",
    "        f.readline()           #rotazioni x x x\n",
    "\n",
    "        line=f.readline()      #numero pezzi x  \n",
    "\n",
    "        #get number of fragments\n",
    "        number=int(re.findall(\"\\d+\",line)[0])\n",
    "\n",
    "        #coupling matrix\n",
    "        grid=[]\n",
    "        for i in range(number):\n",
    "            line=f.readline()\n",
    "            ret=re.findall('-1|0|1',line)\n",
    "            grid.append(list(map(int,ret)))\n",
    "\n",
    "        #model data\n",
    "        model_names=[]\n",
    "        non_valid_indeces=[]\n",
    "        for m in range(number):\n",
    "            f.readline()        #blank line\n",
    "            name=f.readline()   #model name\n",
    "            mesh=f.readline()   #mesh n\n",
    "            f.readline()        #external n\n",
    "            f.readline()        #internal n\n",
    "\n",
    "            #if mesh=0 I don't consider the element\n",
    "            mesh_n=int(mesh.rstrip().split(\" \")[1]) \n",
    "            if mesh_n!=0: \n",
    "                #try create file name\n",
    "                file_name= name.rstrip().replace(\".\",\"_\")\n",
    "                model_names.append(file_name)          \n",
    "            else:\n",
    "                #saving indices to remove later\n",
    "                non_valid_indeces.append(m)\n",
    "\n",
    "        #removing elements from grid\n",
    "        #sorted in reverse to avoid wrong index\n",
    "        for index in sorted(non_valid_indeces, reverse=True):\n",
    "            #remove row\n",
    "            del grid[index]\n",
    "            #remove columns\n",
    "            for row in grid:\n",
    "                del row[index]\n",
    "        \n",
    "        return grid,model_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a set of fragments (i.e. a subfolder)\n",
    "def get_set(folder,verbose=True):\n",
    "    folder_path=os.path.join(main_path,folder)\n",
    "    models_file=[]\n",
    "\n",
    "    #files are either text files or models\n",
    "    for file in os.listdir(folder_path):\n",
    "        if '.txt' in file:\n",
    "            description_file=file\n",
    "        elif '.stl' in file:\n",
    "            #not used, they are not in the same order of the file\n",
    "            models_file.append(file)\n",
    "\n",
    "    #parsing description file\n",
    "    full_description_file=os.path.join(folder_path,description_file)\n",
    "    grid,model_names=process_file(full_description_file)\n",
    "\n",
    "    #get path of models of current set\n",
    "    model_prefix=folder.replace(\"generatedTest_\",\"\")\n",
    "    complete_models_path=[]\n",
    "    for i in range(len(model_names)):\n",
    "        #e.g. 2021_11_29_10_00_40_Cube_001.stl\n",
    "        correct_model_name=f\"{model_prefix}_{model_names[i]}.stl\"\n",
    "        #saving only names, could be useful but now not used\n",
    "        model_names[i]=correct_model_name\n",
    "\n",
    "        complete_models_path.append(os.path.join(folder_path,correct_model_name))\n",
    "\n",
    "    #create set and put into list of sets\n",
    "    set={\"models\":complete_models_path,\"grid\":grid}\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"A set: \\n\")\n",
    "        print(\"Models: \",set[\"models\"])\n",
    "        print(\"Grid: \")\n",
    "        pretty_print(set[\"grid\"])\n",
    "\n",
    "    return set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here i save all the sets of fragments\n",
    "sets=[]\n",
    "\n",
    "#root folder\n",
    "main_path=\"produzione_29112021\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A set: \n",
      "\n",
      "Models:  ['produzione_29112021\\\\generatedTest_2021_11_29_10_00_40\\\\2021_11_29_10_00_40_Cube.stl', 'produzione_29112021\\\\generatedTest_2021_11_29_10_00_40\\\\2021_11_29_10_00_40_Cube_001.stl', 'produzione_29112021\\\\generatedTest_2021_11_29_10_00_40\\\\2021_11_29_10_00_40_Cube_002.stl', 'produzione_29112021\\\\generatedTest_2021_11_29_10_00_40\\\\2021_11_29_10_00_40_Cube_003.stl', 'produzione_29112021\\\\generatedTest_2021_11_29_10_00_40\\\\2021_11_29_10_00_40_Cube_004.stl', 'produzione_29112021\\\\generatedTest_2021_11_29_10_00_40\\\\2021_11_29_10_00_40_Cube_005.stl', 'produzione_29112021\\\\generatedTest_2021_11_29_10_00_40\\\\2021_11_29_10_00_40_Cube_006.stl', 'produzione_29112021\\\\generatedTest_2021_11_29_10_00_40\\\\2021_11_29_10_00_40_Cube_007.stl']\n",
      "Grid: \n",
      "[-1, 1, 0, 0, 1, 1, 1, 1]\n",
      "[1, -1, 1, 0, 1, 1, 1, 0]\n",
      "[0, 1, -1, 1, 1, 1, 1, 0]\n",
      "[0, 0, 1, -1, 1, 0, 1, 0]\n",
      "[1, 1, 1, 1, -1, 0, 1, 1]\n",
      "[1, 1, 1, 0, 0, -1, 1, 0]\n",
      "[1, 1, 1, 1, 1, 1, -1, 1]\n",
      "[1, 0, 0, 0, 1, 0, 1, -1]\n"
     ]
    }
   ],
   "source": [
    "#example set\n",
    "for folder in os.listdir(main_path):\n",
    "    #check only folders, not files\n",
    "    if '.' not in folder:\n",
    "        set1=get_set(folder)\n",
    "        sets.append(set1)\n",
    "        break \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize 3D models and pointCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize some models and point clouds\n",
    "for set in sets:\n",
    "    for path in set[\"models\"]:\n",
    "        mesh = o3d.io.read_triangle_mesh(path)\n",
    "        pointcloud = mesh.sample_points_poisson_disk(1000)\n",
    "\n",
    "        # you can plot and check\n",
    "        #o3d.visualization.draw_geometries([mesh])\n",
    "        #o3d.visualization.draw_geometries([pointcloud])\n",
    "        \n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here i save all the sets of fragments\n",
    "sets=[]\n",
    "\n",
    "#root folder\n",
    "main_path=\"produzione_29112021\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n"
     ]
    }
   ],
   "source": [
    "#Create all the possible pairs of fragments\n",
    "#duplicates are not considered, e.g.: (i,j) - (j,i)\n",
    "def create_pairs(num):\n",
    "    lista = []\n",
    "    for i in range(num):\n",
    "        for j in range(i+1, num):\n",
    "            lista.append((i, j))\n",
    "\n",
    "    return lista\n",
    "\n",
    "print(create_pairs(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(main_path,num_points,max_elements=-1):\n",
    "    \n",
    "    num_elements=0\n",
    "    num_pairs=0\n",
    "\n",
    "    all_data=[]\n",
    "    all_labels=[]\n",
    "    sets=[]\n",
    "    print(\"Creating dataset...\")\n",
    "    print(f\"Each point cloud is sampled with {num_points} points\")\n",
    "    num_totali=0\n",
    "    num_adiacenti=0\n",
    "\n",
    "    for folder in os.listdir(main_path):\n",
    "        if max_elements>0 and num_elements>max_elements:\n",
    "            print(\"maximum number of objects reached, finishing dataset\")\n",
    "            break\n",
    "        #check only folders, not files\n",
    "        if '.' not in folder:\n",
    "            fragment_set=get_set(folder,verbose=False)  #set of fragments of one model\n",
    "            sets.append(fragment_set)\n",
    "            set_pointcloud=[]\n",
    "            for path in fragment_set[\"models\"]:\n",
    "                num_elements+=1\n",
    "                mesh = o3d.io.read_triangle_mesh(path)\n",
    "                pointcloud = mesh.sample_points_poisson_disk(num_points)\n",
    "                set_pointcloud.append(pointcloud)\n",
    "\n",
    "            #get indeces of pairs\n",
    "            pairs=create_pairs(len(fragment_set[\"models\"]))\n",
    "            \n",
    "            for pair in pairs:\n",
    "                num_pairs+=1\n",
    "                idx1,idx2=pair\n",
    "\n",
    "                label=fragment_set[\"grid\"][idx1][idx2]\n",
    "                pair=[ np.asarray(set_pointcloud[idx1].points) ,  np.asarray(set_pointcloud[idx2].points) ]\n",
    "\n",
    "                num_totali+=1\n",
    "                if label==1:\n",
    "                    num_adiacenti+=1\n",
    "\n",
    "                all_data.append(pair)\n",
    "                all_labels.append(label)\n",
    "    print(\"totali: \",num_totali,\" adiacenti: \",num_adiacenti)\n",
    "\n",
    "    print(f\"Dataset contains {num_elements} fragments--> {num_pairs} pairs \")\n",
    "    \n",
    "    return np.array(all_data),np.array(all_labels)\n",
    "\n",
    "#used in class ModelNet40\n",
    "#for now we don't use it\n",
    "def translate_pointcloud(pointcloud):\n",
    "    xyz1 = np.random.uniform(low=2./3., high=3./2., size=[3])\n",
    "    xyz2 = np.random.uniform(low=-0.2, high=0.2, size=[3])\n",
    "       \n",
    "    translated_pointcloud = np.add(np.multiply(pointcloud, xyz1), xyz2).astype('float32')\n",
    "    return translated_pointcloud         \n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,main_path, num_points, partition='train',max_elements=-1):\n",
    "        self.data, self.label = create_dataset(main_path,num_points,max_elements)\n",
    "        print(\"shape of element is: \",self.data.shape)\n",
    "        self.num_points = num_points\n",
    "        self.partition = partition        \n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        pointclouds = self.data[item]\n",
    "        label = self.label[item]\n",
    "        \n",
    "        #if self.partition == 'train':\n",
    "            #pointclouds = translate_pointcloud(pointcloud)\n",
    "            #np.random.shuffle(pointcloud)\n",
    "\n",
    "        return pointclouds, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset...\n",
      "Each point cloud is sampled with 50 points\n",
      "maximum number of objects reached, finishing dataset\n",
      "totali:  28  adiacenti:  19\n",
      "Dataset contains 8 fragments--> 28 pairs \n",
      "shape of element is:  (28, 2, 50, 3)\n",
      "5\n",
      "torch.Size([5, 2, 50, 3])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 2, 50, 3])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 2, 50, 3])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 2, 50, 3])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 2, 50, 3])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "#example dataloader\n",
    "\n",
    "train_loader = DataLoader(MyDataset(partition='train',num_points=50,main_path=main_path,max_elements=2), num_workers=0,\n",
    "                              batch_size=5, shuffle=True, drop_last=True)\n",
    "print(len(train_loader))\n",
    "for data, label in train_loader:\n",
    "    print(data.shape)\n",
    "    print(label.shape)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cal_loss(pred, gold, smoothing=False):\n",
    "    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
    "    #we do not use label smoothing\n",
    "\n",
    "    gold = gold.contiguous().view(-1)\n",
    "\n",
    "    if smoothing:\n",
    "        eps = 0.2\n",
    "        n_class = pred.size(1)\n",
    "\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n",
    "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "        log_prb = F.log_softmax(pred, dim=1)\n",
    "\n",
    "        loss = -(one_hot * log_prb).sum(dim=1).mean()\n",
    "    else:\n",
    "        loss = F.binary_cross_entropy(pred, gold, reduction='mean')\n",
    "\n",
    "    return loss\n",
    "\n",
    "#class to print on file\n",
    "class IOStream():\n",
    "    def __init__(self, path):\n",
    "        self.f = open(path, 'a')\n",
    "\n",
    "    def cprint(self, text):\n",
    "        print(text)\n",
    "        self.f.write(text+'\\n')\n",
    "        self.f.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def knn(x, k):\n",
    "    inner = -2*torch.matmul(x.transpose(2, 1), x)\n",
    "    xx = torch.sum(x**2, dim=1, keepdim=True)\n",
    "    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n",
    " \n",
    "    idx = pairwise_distance.topk(k=k, dim=-1)[1]   # (batch_size, num_points, k)\n",
    "    return idx\n",
    "\n",
    "#aggiunto parametro device in modo da usare cpu se non gpu\n",
    "def get_graph_feature(x, k=20,device=\"cpu\", idx=None, dim9=False):\n",
    "    batch_size = x.size(0)\n",
    "    num_points = x.size(2)\n",
    "    #print(x.shape)\n",
    "    x = x.view(batch_size, -1, num_points)\n",
    "    #print(x.shape)\n",
    "\n",
    "    if idx is None:\n",
    "        if dim9 == False:\n",
    "            idx = knn(x, k=k)   # (batch_size, num_points, k)\n",
    "        else:\n",
    "            idx = knn(x[:, 6:], k=k)\n",
    "    device = torch.device(device)\n",
    "\n",
    "    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1)*num_points\n",
    "\n",
    "    idx = idx + idx_base\n",
    "\n",
    "    idx = idx.view(-1)\n",
    " \n",
    "    _, num_dims, _ = x.size()\n",
    "\n",
    "    x = x.transpose(2, 1).contiguous()   # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n",
    "    feature = x.view(batch_size*num_points, -1)[idx, :]\n",
    "    feature = feature.view(batch_size, num_points, k, num_dims) \n",
    "    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n",
    "    \n",
    "    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n",
    "  \n",
    "    return feature      # (batch_size, 2*num_dims, num_points, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PointNet(nn.Module):\n",
    "    def __init__(self, args, output_channels=40):\n",
    "        super(PointNet, self).__init__()\n",
    "        self.args = args\n",
    "        self.conv1 = nn.Conv1d(3, 64, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n",
    "        self.conv3 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n",
    "        self.conv4 = nn.Conv1d(64, 128, kernel_size=1, bias=False)\n",
    "        self.conv5 = nn.Conv1d(128, args[\"emb_dims\"], kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.bn5 = nn.BatchNorm1d(args[\"emb_dims\"])\n",
    "        self.linear1 = nn.Linear(args[\"emb_dims\"], 512, bias=False)\n",
    "        self.bn6 = nn.BatchNorm1d(512)\n",
    "        self.dp1 = nn.Dropout()\n",
    "        self.linear2 = nn.Linear(512, output_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = F.adaptive_max_pool1d(x, 1).squeeze()\n",
    "        x = F.relu(self.bn6(self.linear1(x)))\n",
    "        x = self.dp1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DGCNN_cls(nn.Module):\n",
    "    def __init__(self, args, output_channels=1):\n",
    "        super(DGCNN_cls, self).__init__()\n",
    "        self.args = args\n",
    "        self.device=\"cuda\" if self.args[\"cuda\"] else \"cpu\"\n",
    "        self.k = args[\"k\"]\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.bn5 = nn.BatchNorm1d(args[\"emb_dims\"])\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn1,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn2,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 128, kernel_size=1, bias=False),\n",
    "                                   self.bn3,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(128*2, 256, kernel_size=1, bias=False),\n",
    "                                   self.bn4,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv5 = nn.Sequential(nn.Conv1d(512, args[\"emb_dims\"], kernel_size=1, bias=False),\n",
    "                                   self.bn5,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.linear1 = nn.Linear(args[\"emb_dims\"]*2, 512, bias=False)\n",
    "        self.bn6 = nn.BatchNorm1d(512)\n",
    "        self.dp1 = nn.Dropout(p=args[\"dropout\"])\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.bn7 = nn.BatchNorm1d(256)\n",
    "        self.dp2 = nn.Dropout(p=args[\"dropout\"])\n",
    "        self.linear3 = nn.Linear(256, 32)\n",
    "\n",
    "        #final classification layer\n",
    "        self.linear_classification = nn.Linear(64, output_channels)\n",
    "\n",
    "\n",
    "    \n",
    "    def step(self,x):\n",
    "        batch_size = x.size(0)\n",
    "        x = get_graph_feature(x, k=self.k,device=self.device)      # (batch_size, 3, num_points) -> (batch_size, 3*2, num_points, k)\n",
    "        x = self.conv1(x)                       # (batch_size, 3*2, num_points, k) -> (batch_size, 64, num_points, k)\n",
    "        x1 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n",
    "\n",
    "        x = get_graph_feature(x1, k=self.k,device=self.device)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n",
    "        x = self.conv2(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n",
    "        x2 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n",
    "\n",
    "        x = get_graph_feature(x2, k=self.k,device=self.device)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n",
    "        x = self.conv3(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 128, num_points, k)\n",
    "        x3 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 128, num_points, k) -> (batch_size, 128, num_points)\n",
    "\n",
    "        x = get_graph_feature(x3, k=self.k,device=self.device)     # (batch_size, 128, num_points) -> (batch_size, 128*2, num_points, k)\n",
    "        x = self.conv4(x)                       # (batch_size, 128*2, num_points, k) -> (batch_size, 256, num_points, k)\n",
    "        x4 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 256, num_points, k) -> (batch_size, 256, num_points)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4), dim=1)  # (batch_size, 64+64+128+256, num_points)\n",
    "\n",
    "        x = self.conv5(x)                       # (batch_size, 64+64+128+256, num_points) -> (batch_size, emb_dims, num_points)\n",
    "        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)           # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims)\n",
    "        x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)           # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims)\n",
    "        x = torch.cat((x1, x2), 1)              # (batch_size, emb_dims*2)\n",
    "\n",
    "        x = F.leaky_relu(self.bn6(self.linear1(x)), negative_slope=0.2) # (batch_size, emb_dims*2) -> (batch_size, 512)\n",
    "        x = self.dp1(x)\n",
    "        x = F.leaky_relu(self.bn7(self.linear2(x)), negative_slope=0.2) # (batch_size, 512) -> (batch_size, 256)\n",
    "        x = self.dp2(x)\n",
    "        x = self.linear3(x)                                             # (batch_size, 256) -> (batch_size, output_channels)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        #first pointcloud\n",
    "        first=x[:,0,:,:]\n",
    "        second=x[:,1,:,:]\n",
    "\n",
    "        #compute DGCNN output from both pointclouds \n",
    "        ret1=self.step(first)      \n",
    "        ret2=self.step(second)\n",
    "\n",
    "        #combine output and use classifier\n",
    "        ret=torch.cat((ret1,ret2),dim=1)\n",
    "        x=self.linear_classification(ret)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_():\n",
    "    if not os.path.exists('outputs'):\n",
    "        os.makedirs('outputs')\n",
    "    if not os.path.exists('outputs/'+args[\"exp_name\"]):\n",
    "        os.makedirs('outputs/'+args[\"exp_name\"])\n",
    "    if not os.path.exists('outputs/'+args[\"exp_name\"]+'/'+'models'):\n",
    "        os.makedirs('outputs/'+args[\"exp_name\"]+'/'+'models')\n",
    "    os.system('cp main_cls.py outputs'+'/'+args[\"exp_name\"]+'/'+'main_cls.py.backup')\n",
    "    os.system('cp model.py outputs' + '/' + args[\"exp_name\"] + '/' + 'model.py.backup')\n",
    "    os.system('cp util.py outputs' + '/' + args[\"exp_name\"] + '/' + 'util.py.backup')\n",
    "    os.system('cp data.py outputs' + '/' + args[\"exp_name\"] + '/' + 'data.py.backup')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(args, io):\n",
    "    # num_workers=0 to make it work on windows\n",
    "    train_loader = DataLoader(MyDataset(main_path, num_points=args[\"num_points\"], partition='train',max_elements=args[\"max_elements\"]), num_workers=0,\n",
    "                              batch_size=args[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(MyDataset(main_path, num_points=args[\"num_points\"], partition='test',max_elements=args[\"max_elements\"]), num_workers=0,\n",
    "                             batch_size=args[\"test_batch_size\"], shuffle=True, drop_last=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if args[\"cuda\"] else \"cpu\")\n",
    "\n",
    "    #Try to load models\n",
    "    if args[\"model\"] == 'pointnet':\n",
    "        model = PointNet(args).to(device)\n",
    "    elif args[\"model\"] == 'dgcnn':\n",
    "        model = DGCNN_cls(args).to(device)\n",
    "    else:\n",
    "        raise Exception(\"Not implemented\")\n",
    "\n",
    "    print(\"Model loaded!\")\n",
    "    #print(str(model))\n",
    "\n",
    "    model = nn.DataParallel(model)\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "\n",
    "    if args[\"use_sgd\"]:\n",
    "        print(\"Use SGD\")\n",
    "        opt = optim.SGD(model.parameters(), lr=args[\"lr\"]*100,\n",
    "                momentum=args[\"momentum\"], weight_decay=1e-4)\n",
    "    else:\n",
    "        print(\"Use Adam\")\n",
    "        opt = optim.Adam(model.parameters(), lr=args[\"lr\"], weight_decay=1e-4)\n",
    "\n",
    "    if args[\"scheduler\"] == 'cos':\n",
    "        scheduler = CosineAnnealingLR(opt, args[\"epochs\"], eta_min=1e-3)\n",
    "    elif args[\"scheduler\"] == 'step':\n",
    "        scheduler = StepLR(opt, step_size=20, gamma=0.7)\n",
    "    \n",
    "    criterion = cal_loss\n",
    "\n",
    "    best_test_acc = 0\n",
    "    print(args)\n",
    "    for epoch in range(args[\"epochs\"]):\n",
    "        ####################\n",
    "        # Train\n",
    "        ####################\n",
    "        train_loss = 0.0\n",
    "        count = 0.0\n",
    "        model.train()\n",
    "        train_pred = []\n",
    "        train_true = []\n",
    "        print(\"Starting epoch \",epoch)\n",
    "        for data, label in train_loader:\n",
    "            #print(\"get data\")\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            #print(label)\n",
    "            data=data.float()\n",
    "            label=label.float()\n",
    "            data = data.permute(0,1,3,2)\n",
    "            batch_size = data.size()[0]\n",
    "            opt.zero_grad()\n",
    "            logits = model(data)\n",
    "            logits=torch.squeeze(torch.sigmoid(logits))\n",
    "            loss = criterion(logits, label)\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            #preds = logits.max(dim=1)[1] #not needed, we have only one value for each sample\n",
    "            preds=logits.int()\n",
    "            count += batch_size\n",
    "            train_loss += loss.item() * batch_size\n",
    "            train_true.append(label.cpu().numpy())\n",
    "            train_pred.append(preds.detach().cpu().numpy())\n",
    "        \n",
    "        if args[\"scheduler\"] == 'cos':\n",
    "            scheduler.step()\n",
    "        elif args[\"scheduler\"] == 'step':\n",
    "            if opt.param_groups[0]['lr'] > 1e-5:\n",
    "                scheduler.step()\n",
    "            if opt.param_groups[0]['lr'] < 1e-5:\n",
    "                for param_group in opt.param_groups:\n",
    "                    param_group['lr'] = 1e-5\n",
    "\n",
    "        \n",
    "        train_true = np.concatenate(train_true)\n",
    "        train_pred = np.concatenate(train_pred)\n",
    "        outstr = 'Train %d, loss: %.6f, train acc: %.6f, train avg acc: %.6f' % (epoch,\n",
    "                                                                                 train_loss*1.0/count,\n",
    "                                                                                 metrics.accuracy_score(\n",
    "                                                                                     train_true, train_pred),\n",
    "                                                                                 metrics.balanced_accuracy_score(\n",
    "                                                                                     train_true, train_pred))\n",
    "        io.cprint(outstr)\n",
    "\n",
    "        ####################\n",
    "        # Test\n",
    "        ####################\n",
    "        test_loss = 0.0\n",
    "        count = 0.0\n",
    "        model.eval()\n",
    "        test_pred = []\n",
    "        test_true = []\n",
    "        for data, label in test_loader:\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            data=data.float()\n",
    "            label=label.float()\n",
    "\n",
    "            data = data.permute(0,1,3,2)\n",
    "            batch_size = data.size()[0]\n",
    "            logits = model(data)\n",
    "            logits=torch.squeeze(torch.sigmoid(logits))\n",
    "            loss = criterion(logits, label)\n",
    "            \n",
    "            #preds = logits.max(dim=1)[1] #not needed, we have only one value for each sample\n",
    "            preds=logits.int()\n",
    "\n",
    "            count += batch_size\n",
    "            test_loss += loss.item() * batch_size\n",
    "            test_true.append(label.cpu().numpy())\n",
    "            test_pred.append(preds.detach().cpu().numpy())\n",
    "        test_true = np.concatenate(test_true)\n",
    "        test_pred = np.concatenate(test_pred)\n",
    "        test_acc = metrics.accuracy_score(test_true, test_pred)\n",
    "        avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n",
    "        outstr = 'Test %d, loss: %.6f, test acc: %.6f, test avg acc: %.6f' % (epoch,\n",
    "                                                                              test_loss*1.0/count,\n",
    "                                                                              test_acc,\n",
    "                                                                              avg_per_class_acc)\n",
    "        io.cprint(outstr)\n",
    "\n",
    "        ''' \n",
    "        #save model does not work, need file created, to do\n",
    "        if test_acc >= best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            torch.save(model.state_dict(), 'outputs/%s/models/model.t7' % args[\"use_sgd\"])'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(args, io):\n",
    "    test_loader = DataLoader(MyDataset(partition='test', num_points=args[\"num_points\"]),\n",
    "                             batch_size=args[\"test_batch_size\"], shuffle=True, drop_last=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if args[\"cuda\"] else \"cpu\")\n",
    "\n",
    "    #Try to load models\n",
    "    if args[\"mdoel\"] == 'pointnet':\n",
    "        model = PointNet(args).to(device)\n",
    "    elif args[\"mdoel\"] == 'dgcnn':\n",
    "        model = DGCNN_cls(args).to(device)\n",
    "    else:\n",
    "        raise Exception(\"Not implemented\")\n",
    "\n",
    "    model = nn.DataParallel(model)\n",
    "    \n",
    "    model.load_state_dict(torch.load(args[\"model_path\"]))\n",
    "    model = model.eval()\n",
    "    test_acc = 0.0\n",
    "    count = 0.0\n",
    "    test_true = []\n",
    "    test_pred = []\n",
    "    for data, label in test_loader:\n",
    "\n",
    "        data, label = data.to(device), label.to(device).squeeze()\n",
    "        data = data.permute(0, 2, 1)\n",
    "        batch_size = data.size()[0]\n",
    "        logits = model(data)\n",
    "        #preds = logits.max(dim=1)[1]\n",
    "        preds=logits.int()\n",
    "        test_true.append(label.cpu().numpy())\n",
    "        test_pred.append(preds.detach().cpu().numpy())\n",
    "    test_true = np.concatenate(test_true)\n",
    "    test_pred = np.concatenate(test_pred)\n",
    "    test_acc = metrics.accuracy_score(test_true, test_pred)\n",
    "    avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n",
    "    outstr = 'Test :: test acc: %.6f, test avg acc: %.6f'%(test_acc, avg_per_class_acc)\n",
    "    io.cprint(outstr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exp_name': 'exp', 'model': 'dgcnn', 'dataset': 'modelnet40', 'batch_size': 16, 'test_batch_size': 16, 'epochs': 50, 'use_sgd': False, 'lr': 0.001, 'momentum': 0.9, 'scheduler': 'cos', 'no_cuda': False, 'seed': 1, 'eval': False, 'num_points': 100, 'dropout': 0.5, 'emb_dims': 1024, 'k': 20, 'model_path': '', 'max_elements': 30}\n",
      "Using CPU\n",
      "Creating dataset...\n",
      "Each point cloud is sampled with 100 points\n",
      "maximum number of objects reached, finishing dataset\n",
      "Dataset contains 51 fragments--> 435 pairs \n",
      "Creating dataset...\n",
      "Each point cloud is sampled with 100 points\n",
      "maximum number of objects reached, finishing dataset\n",
      "Dataset contains 51 fragments--> 435 pairs \n",
      "Model loaded!\n",
      "Let's use 0 GPUs!\n",
      "Use Adam\n",
      "{'exp_name': 'exp', 'model': 'dgcnn', 'dataset': 'modelnet40', 'batch_size': 16, 'test_batch_size': 16, 'epochs': 50, 'use_sgd': False, 'lr': 0.001, 'momentum': 0.9, 'scheduler': 'cos', 'no_cuda': False, 'seed': 1, 'eval': False, 'num_points': 100, 'dropout': 0.5, 'emb_dims': 1024, 'k': 20, 'model_path': '', 'max_elements': 30, 'cuda': False}\n",
      "Starting epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alessio\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0, loss: 0.620520, train acc: 0.648148, train avg acc: 0.500000\n",
      "Test 0, loss: 0.589304, test acc: 0.648276, test avg acc: 0.500000\n",
      "Starting epoch  1\n",
      "Train 1, loss: 0.587883, train acc: 0.648148, train avg acc: 0.500000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a3a8442492dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"eval\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-0247a69dd754>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args, io)\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-5320e1868218>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;31m#compute DGCNN output from both pointclouds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mret1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[0mret2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msecond\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-5320e1868218>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_graph_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# (batch_size, 128, num_points) -> (batch_size, 128*2, num_points, k)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;31m# (batch_size, 128*2, num_points, k) -> (batch_size, 256, num_points, k)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mx4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m    \u001b[1;31m# (batch_size, 256, num_points, k) -> (batch_size, 256, num_points)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 340\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    args={\n",
    "        \"exp_name\": \"exp\",          # Name of the experiment\n",
    "        \"model\": \"dgcnn\",           # Model to use, [pointnet, dgcnn]\n",
    "        \"dataset\": \"modelnet40\",    # Dataset to use, we don't use this\n",
    "        \"batch_size\": 16,           # Size of batch\n",
    "        \"test_batch_size\": 16,      # Size of batch\n",
    "        \"epochs\": 50,              # number of episode to train\n",
    "        \"use_sgd\": False,            # Use SGD\n",
    "        \"lr\": 0.001,                # learning rate (default: 0.001, 0.1 if using sgd)\n",
    "        \"momentum\": 0.9,            # SGD momentum (default: 0.9)\n",
    "        \"scheduler\": \"cos\",         # Scheduler to use, [cos, step]\n",
    "        \"no_cuda\": False,           # enables CUDA training\n",
    "        \"seed\": 1,                  # random seed (default: 1)\n",
    "        \"eval\": False,              # evaluate the model\n",
    "        \"num_points\": 100,         # num of points to use\n",
    "        \"dropout\": 0.5,             # initial dropout rate\n",
    "        \"emb_dims\": 1024,           # Dimension of embeddings\n",
    "        \"k\": 20,                    # Num of nearest neighbors to use\n",
    "        \"model_path\": \"\",           # Pretrained model path\n",
    "        \"max_elements\": 30,         # Num of 3D fragments to consider\n",
    "    }\n",
    "\n",
    "    _init_()\n",
    "\n",
    "    io = IOStream('outputs/' + args[\"exp_name\"] + '/run.log')\n",
    "    io.cprint(str(args))\n",
    "    \n",
    "    #set up GPU\n",
    "    args[\"cuda\"] = not args[\"no_cuda\"] and torch.cuda.is_available()\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "    if args[\"cuda\"]:\n",
    "        io.cprint(\n",
    "            'Using GPU : ' + str(torch.cuda.current_device()) + ' from ' + str(torch.cuda.device_count()) + ' devices')\n",
    "        torch.cuda.manual_seed(args[\"seed\"])\n",
    "    else:\n",
    "        io.cprint('Using CPU')\n",
    "\n",
    "    if not args[\"eval\"]:\n",
    "        train(args, io)\n",
    "    else:\n",
    "        test(args, io)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "691f1b564724e08df0ae28181cdfea598ff9582372b176e10f27b62d8e6609e2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
