{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.init as init\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "#insert here wandb key\n",
    "WANDB_KEY=\"\"\n",
    "if WANDB_KEY:\n",
    "    wandb.login(key=WANDB_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,args,partition='train'):\n",
    "        self.args=args\n",
    "        self.dataset_file=h5py.File(self.args[\"dataset_file_path\"], 'r')\n",
    "        self.num_train=min(args[\"num_train_samples\"],len(self.dataset_file[\"train_data\"]))\n",
    "        self.num_test=min(args[\"num_test_samples\"],len(self.dataset_file[\"test_data\"]))\n",
    "\n",
    "        if args[\"dataset_in_ram\"]:\n",
    "            self.train_set=self.dataset_file[\"train_data\"][:self.num_train]\n",
    "            self.train_labels=self.dataset_file[\"train_labels\"][:self.num_train]\n",
    "            self.test_set=self.dataset_file[\"test_data\"][:self.num_test]\n",
    "            self.test_labels=self.dataset_file[\"test_labels\"][:self.num_test]\n",
    "\n",
    "        self.partition = partition        \n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.partition==\"train\":\n",
    "            if self.args[\"dataset_in_ram\"]:\n",
    "                pointclouds_pair=self.train_set[item]\n",
    "                label=self.train_labels[item]\n",
    "            else:\n",
    "                pointclouds_pair=self.dataset_file[\"train_data\"][item]\n",
    "                label=self.dataset_file[\"train_labels\"][item]\n",
    "        else:\n",
    "            if self.args[\"dataset_in_ram\"]:\n",
    "                pointclouds_pair=self.test_set[item]\n",
    "                label=self.test_labels[item]\n",
    "            else:\n",
    "                pointclouds_pair=self.dataset_file[\"test_data\"][item]\n",
    "                label=self.dataset_file[\"test_labels\"][item]\n",
    "        return pointclouds_pair, label\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.partition==\"train\":\n",
    "            return self.num_train\n",
    "        else:\n",
    "            return self.num_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cal_loss(pred, gold, smoothing=False):\n",
    "    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
    "    #we do not use label smoothing\n",
    "\n",
    "    gold = gold.contiguous().view(-1)\n",
    "\n",
    "    if smoothing:\n",
    "        eps = 0.2\n",
    "        n_class = pred.size(1)\n",
    "\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n",
    "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "        log_prb = F.log_softmax(pred, dim=1)\n",
    "\n",
    "        loss = -(one_hot * log_prb).sum(dim=1).mean()\n",
    "    else:\n",
    "        loss = F.binary_cross_entropy(pred, gold, reduction='mean')\n",
    "\n",
    "    return loss\n",
    "\n",
    "#class to print on file\n",
    "class IOStream():\n",
    "    def __init__(self, path):\n",
    "        self.f = open(path, 'a')\n",
    "\n",
    "    def cprint(self, text):\n",
    "        print(text)\n",
    "        self.f.write(text+'\\n')\n",
    "        self.f.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def knn(x, k):\n",
    "    inner = -2*torch.matmul(x.transpose(2, 1), x)\n",
    "    xx = torch.sum(x**2, dim=1, keepdim=True)\n",
    "    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n",
    " \n",
    "    idx = pairwise_distance.topk(k=k, dim=-1)[1]   # (batch_size, num_points, k)\n",
    "    return idx\n",
    "\n",
    "#aggiunto parametro device in modo da usare cpu se non gpu\n",
    "def get_graph_feature(x, k=20,device=\"cpu\", idx=None, dim9=False):\n",
    "    batch_size = x.size(0)\n",
    "    num_points = x.size(2)\n",
    "    #print(x.shape)\n",
    "    x = x.view(batch_size, -1, num_points)\n",
    "    #print(x.shape)\n",
    "\n",
    "    if idx is None:\n",
    "        if dim9 == False:\n",
    "            idx = knn(x, k=k)   # (batch_size, num_points, k)\n",
    "        else:\n",
    "            idx = knn(x[:, 6:], k=k)\n",
    "    device = torch.device(device)\n",
    "\n",
    "    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1)*num_points\n",
    "\n",
    "    idx = idx + idx_base\n",
    "\n",
    "    idx = idx.view(-1)\n",
    " \n",
    "    _, num_dims, _ = x.size()\n",
    "\n",
    "    x = x.transpose(2, 1).contiguous()   # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n",
    "    feature = x.view(batch_size*num_points, -1)[idx, :]\n",
    "    feature = feature.view(batch_size, num_points, k, num_dims) \n",
    "    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n",
    "    \n",
    "    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n",
    "  \n",
    "    return feature      # (batch_size, 2*num_dims, num_points, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DGCNN_cls(nn.Module):\n",
    "    def __init__(self, args, output_channels=1):\n",
    "        super(DGCNN_cls, self).__init__()\n",
    "        self.args = args\n",
    "        self.device=\"cuda\" if self.args[\"cuda\"] else \"cpu\"\n",
    "        self.k = args[\"k\"]\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.bn5 = nn.BatchNorm1d(args[\"emb_dims\"])\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn1,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn2,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 128, kernel_size=1, bias=False),\n",
    "                                   self.bn3,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(128*2, 256, kernel_size=1, bias=False),\n",
    "                                   self.bn4,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv5 = nn.Sequential(nn.Conv1d(512, args[\"emb_dims\"], kernel_size=1, bias=False),\n",
    "                                   self.bn5,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.linear1 = nn.Linear(args[\"emb_dims\"]*2, 512, bias=False)\n",
    "        self.bn6 = nn.BatchNorm1d(512)\n",
    "        self.dp1 = nn.Dropout(p=args[\"dropout\"])\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.bn7 = nn.BatchNorm1d(256)\n",
    "        self.dp2 = nn.Dropout(p=args[\"dropout\"])\n",
    "        self.linear3 = nn.Linear(256, args[\"final_dim_DGCNN\"])\n",
    "\n",
    "        #final classification layer\n",
    "        if args[\"dim_classification\"]>0:\n",
    "            self.beginClassification = nn.Linear(args[\"final_dim_DGCNN\"]*2, args[\"dim_classification\"])\n",
    "            self.midClassification = nn.Linear(args[\"dim_classification\"], args[\"dim_classification\"])\n",
    "            self.endClassification = nn.Linear(args[\"dim_classification\"] , output_channels)\n",
    "\n",
    "        else:\n",
    "            self.classification= nn.Linear(args[\"final_dim_DGCNN\"]*2, output_channels)\n",
    "\n",
    "\n",
    "    \n",
    "    def dgcnn_step(self,x):\n",
    "        batch_size = x.size(0)\n",
    "        x = get_graph_feature(x, k=self.k,device=self.device)      # (batch_size, 3, num_points) -> (batch_size, 3*2, num_points, k)\n",
    "        x = self.conv1(x)                       # (batch_size, 3*2, num_points, k) -> (batch_size, 64, num_points, k)\n",
    "        x1 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n",
    "\n",
    "        x = get_graph_feature(x1, k=self.k,device=self.device)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n",
    "        x = self.conv2(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n",
    "        x2 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n",
    "\n",
    "        x = get_graph_feature(x2, k=self.k,device=self.device)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n",
    "        x = self.conv3(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 128, num_points, k)\n",
    "        x3 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 128, num_points, k) -> (batch_size, 128, num_points)\n",
    "\n",
    "        x = get_graph_feature(x3, k=self.k,device=self.device)     # (batch_size, 128, num_points) -> (batch_size, 128*2, num_points, k)\n",
    "        x = self.conv4(x)                       # (batch_size, 128*2, num_points, k) -> (batch_size, 256, num_points, k)\n",
    "        x4 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 256, num_points, k) -> (batch_size, 256, num_points)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4), dim=1)  # (batch_size, 64+64+128+256, num_points)\n",
    "\n",
    "        x = self.conv5(x)                       # (batch_size, 64+64+128+256, num_points) -> (batch_size, emb_dims, num_points)\n",
    "        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)           # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims)\n",
    "        x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)           # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims)\n",
    "        x = torch.cat((x1, x2), 1)              # (batch_size, emb_dims*2)\n",
    "\n",
    "        x = F.leaky_relu(self.bn6(self.linear1(x)), negative_slope=0.2) # (batch_size, emb_dims*2) -> (batch_size, 512)\n",
    "        x = self.dp1(x)\n",
    "        x = F.leaky_relu(self.bn7(self.linear2(x)), negative_slope=0.2) # (batch_size, 512) -> (batch_size, 256)\n",
    "        x = self.dp2(x)\n",
    "        x = self.linear3(x)                                             # (batch_size, 256) -> (batch_size, output_channels)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        #first pointcloud\n",
    "        first=x[:,0,:,:]\n",
    "        second=x[:,1,:,:]\n",
    "\n",
    "        #compute DGCNN output from both pointclouds \n",
    "        ret1=self.dgcnn_step(first)      \n",
    "        ret2=self.dgcnn_step(second)\n",
    "\n",
    "        #combine output and use classifier\n",
    "        x=torch.cat((ret1,ret2),dim=1)\n",
    "        if self.args[\"dim_classification\"]>0:\n",
    "            x=self.beginClassification(x)\n",
    "            x=self.midClassification(x)\n",
    "            x=self.endClassification(x)\n",
    "        else: \n",
    "            x=self.classification(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(args, io):\n",
    "    # num_workers=0 to make it work on windows\n",
    "    train_loader = DataLoader(MyDataset(args,partition='train'), \n",
    "            num_workers=args[\"num_workers\"],batch_size=args[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(MyDataset(args,partition='test'), \n",
    "            num_workers=args[\"num_workers\"],batch_size=args[\"test_batch_size\"], shuffle=True, drop_last=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if args[\"cuda\"] else \"cpu\")\n",
    "\n",
    "    model = DGCNN_cls(args).to(device)\n",
    "\n",
    "    print(\"Model loaded!\")\n",
    "\n",
    "    model = nn.DataParallel(model)\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "\n",
    "    if args[\"use_sgd\"]:\n",
    "        print(\"Use SGD\")\n",
    "        opt = optim.SGD(model.parameters(), lr=args[\"lr\"]*100,\n",
    "                momentum=args[\"momentum\"], weight_decay=1e-4)\n",
    "    else:\n",
    "        print(\"Use Adam\")\n",
    "        opt = optim.Adam(model.parameters(), lr=args[\"lr\"], weight_decay=1e-4)\n",
    "\n",
    "    if args[\"scheduler\"] == 'cos':\n",
    "        scheduler = CosineAnnealingLR(opt, args[\"epochs\"], eta_min=1e-3)\n",
    "    elif args[\"scheduler\"] == 'step':\n",
    "        scheduler = StepLR(opt, step_size=20, gamma=0.7)\n",
    "    \n",
    "    criterion = cal_loss\n",
    "\n",
    "    if args[\"wand_project_name\"]:\n",
    "            wandb.init(\n",
    "        # Set the project where this run will be logged\n",
    "        project=args[\"wand_project_name\"], \n",
    "        # Track hyperparameters and run metadata\n",
    "        config=args)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(args)\n",
    "    #\n",
    "    # TRAINING\n",
    "    #\n",
    "    for epoch in range(args[\"epochs\"]):\n",
    "        train_loss = 0.0\n",
    "        count = 0.0\n",
    "        model.train()\n",
    "        train_pred = []\n",
    "        train_true = []\n",
    "        print(\"Starting epoch \",epoch)\n",
    "        correct_samples=0\n",
    "        total_samples=0\n",
    "        for data, label in train_loader:\n",
    "            #print(\"get data\")\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            #print(label)\n",
    "            data=data.float()\n",
    "            label=label.float()\n",
    "            data = data.permute(0,1,3,2)\n",
    "            batch_size = data.size()[0]\n",
    "            opt.zero_grad()\n",
    "            logits = model(data)\n",
    "            logits=torch.squeeze(torch.sigmoid(logits))\n",
    "            loss = criterion(logits, label)\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            #preds = logits.max(dim=1)[1] #not needed, we have only one value for each sample\n",
    "            preds=(logits>0.5).int()\n",
    "            count += batch_size\n",
    "            train_loss += loss.item() * batch_size\n",
    "            train_true.append(label.cpu().numpy())\n",
    "            for i in range(len(preds)):\n",
    "                total_samples+=1\n",
    "                if preds[i]==label[i]:\n",
    "                    correct_samples+=1\n",
    "            train_pred.append(preds.detach().cpu().numpy())\n",
    "        \n",
    "        if args[\"scheduler\"] == 'cos':\n",
    "            scheduler.step()\n",
    "        elif args[\"scheduler\"] == 'step':\n",
    "            if opt.param_groups[0]['lr'] > 1e-5:\n",
    "                scheduler.step()\n",
    "            if opt.param_groups[0]['lr'] < 1e-5:\n",
    "                for param_group in opt.param_groups:\n",
    "                    param_group['lr'] = 1e-5\n",
    "\n",
    "        \n",
    "        train_true = np.concatenate(train_true)\n",
    "        train_pred = np.concatenate(train_pred)\n",
    "        print(\"my accuracy: \", correct_samples/total_samples)\n",
    "        train_acc=metrics.accuracy_score(train_true, train_pred)\n",
    "        train_avg_acc= metrics.balanced_accuracy_score(train_true, train_pred)\n",
    "        wandb.log({\"train_acc\": train_acc,\"train_avg_acc\": train_avg_acc, \"train_loss\": loss})\n",
    "        outstr = 'Train %d, loss: %.6f, train acc: %.6f, train avg acc: %.6f' % (epoch,train_loss*1.0/count,train_acc,train_avg_acc)\n",
    "        io.cprint(outstr)\n",
    "\n",
    "        ####################\n",
    "        # Test\n",
    "        ####################\n",
    "        test_loss = 0.0\n",
    "        count = 0.0\n",
    "        model.eval()\n",
    "        test_pred = []\n",
    "        test_true = []\n",
    "        for data, label in test_loader:\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            data=data.float()\n",
    "            label=label.float()\n",
    "\n",
    "            data = data.permute(0,1,3,2)\n",
    "            batch_size = data.size()[0]\n",
    "            logits = model(data)\n",
    "            logits=torch.squeeze(torch.sigmoid(logits))\n",
    "            loss = criterion(logits, label)\n",
    "            \n",
    "            #preds = logits.max(dim=1)[1] #not needed, we have only one value for each sample\n",
    "            preds=(logits>0.5).int()\n",
    "\n",
    "            count += batch_size\n",
    "            test_loss += loss.item() * batch_size\n",
    "            test_true.append(label.cpu().numpy())\n",
    "            test_pred.append(preds.detach().cpu().numpy())\n",
    "        test_true = np.concatenate(test_true)\n",
    "        test_pred = np.concatenate(test_pred)\n",
    "        test_acc = metrics.accuracy_score(test_true, test_pred)\n",
    "        test_avg_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n",
    "        wandb.log({\"test_acc\": test_acc,\"test_avg_acc\": test_avg_acc, \"test_loss\": loss})\n",
    "        outstr = 'Test %d, loss: %.6f, test acc: %.6f, test avg acc: %.6f' % (epoch,\n",
    "                                                                              test_loss*1.0/count,\n",
    "                                                                              test_acc,\n",
    "                                                                              test_avg_acc)\n",
    "        io.cprint(outstr)\n",
    "        #end epoch\n",
    "    wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(args, io):\n",
    "    test_loader = DataLoader(MyDataset(dataset_file_name=args[\"dataset_file_path\"],partition='test',num_points=args[\"num_points\"]), \n",
    "            num_workers=args[\"num_workers\"],batch_size=args[\"test_batch_size\"], shuffle=True, drop_last=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if args[\"cuda\"] else \"cpu\")\n",
    "\n",
    "    model = DGCNN_cls(args).to(device)\n",
    "\n",
    "    model = nn.DataParallel(model)\n",
    "    \n",
    "    model.load_state_dict(torch.load(args[\"model_path\"]))\n",
    "    model = model.eval()\n",
    "    test_acc = 0.0\n",
    "    count = 0.0\n",
    "    test_true = []\n",
    "    test_pred = []\n",
    "    for data, label in test_loader:\n",
    "\n",
    "        data, label = data.to(device), label.to(device).squeeze()\n",
    "        data = data.permute(0, 2, 1)\n",
    "        batch_size = data.size()[0]\n",
    "        logits = model(data)\n",
    "        #preds = logits.max(dim=1)[1]\n",
    "        preds=(logits>0.5).int()\n",
    "        test_true.append(label.cpu().numpy())\n",
    "        test_pred.append(preds.detach().cpu().numpy())\n",
    "    test_true = np.concatenate(test_true)\n",
    "    test_pred = np.concatenate(test_pred)\n",
    "    test_acc = metrics.accuracy_score(test_true, test_pred)\n",
    "    avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n",
    "    outstr = 'Test :: test acc: %.6f, test avg acc: %.6f'%(test_acc, avg_per_class_acc)\n",
    "    io.cprint(outstr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exp_name': 'exp', 'model': 'dgcnn', 'dataset': 'modelnet40', 'batch_size': 16, 'test_batch_size': 16, 'epochs': 50, 'use_sgd': False, 'lr': 0.001, 'momentum': 0.9, 'scheduler': 'cos', 'no_cuda': False, 'seed': 1, 'eval': False, 'num_points': 100, 'dropout': 0.5, 'emb_dims': 1024, 'k': 20, 'model_path': '', 'max_elements': 30}\n",
      "Using CPU\n",
      "Creating dataset...\n",
      "Each point cloud is sampled with 100 points\n",
      "maximum number of objects reached, finishing dataset\n",
      "Dataset contains 51 fragments--> 435 pairs \n",
      "Creating dataset...\n",
      "Each point cloud is sampled with 100 points\n",
      "maximum number of objects reached, finishing dataset\n",
      "Dataset contains 51 fragments--> 435 pairs \n",
      "Model loaded!\n",
      "Let's use 0 GPUs!\n",
      "Use Adam\n",
      "{'exp_name': 'exp', 'model': 'dgcnn', 'dataset': 'modelnet40', 'batch_size': 16, 'test_batch_size': 16, 'epochs': 50, 'use_sgd': False, 'lr': 0.001, 'momentum': 0.9, 'scheduler': 'cos', 'no_cuda': False, 'seed': 1, 'eval': False, 'num_points': 100, 'dropout': 0.5, 'emb_dims': 1024, 'k': 20, 'model_path': '', 'max_elements': 30, 'cuda': False}\n",
      "Starting epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alessio\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0, loss: 0.620520, train acc: 0.648148, train avg acc: 0.500000\n",
      "Test 0, loss: 0.589304, test acc: 0.648276, test avg acc: 0.500000\n",
      "Starting epoch  1\n",
      "Train 1, loss: 0.587883, train acc: 0.648148, train avg acc: 0.500000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a3a8442492dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"eval\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-0247a69dd754>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args, io)\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-5320e1868218>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;31m#compute DGCNN output from both pointclouds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mret1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[0mret2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msecond\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-5320e1868218>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_graph_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# (batch_size, 128, num_points) -> (batch_size, 128*2, num_points, k)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;31m# (batch_size, 128*2, num_points, k) -> (batch_size, 256, num_points, k)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mx4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m    \u001b[1;31m# (batch_size, 256, num_points, k) -> (batch_size, 256, num_points)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 340\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def _init_():\n",
    "    if not os.path.exists('outputs'):\n",
    "        os.makedirs('outputs')\n",
    "    if not os.path.exists('outputs/'+args[\"exp_name\"]):\n",
    "        os.makedirs('outputs/'+args[\"exp_name\"])\n",
    "    if not os.path.exists('outputs/'+args[\"exp_name\"]+'/'+'models'):\n",
    "        os.makedirs('outputs/'+args[\"exp_name\"]+'/'+'models')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    args={\n",
    "        \"exp_name\": \"exp\",\n",
    "        \"wand_project_name\": \"Fragment_EAI\",\n",
    "        #dataset\n",
    "        \"dataset_file_path\": \"dataset_90810pairs_100points.h5py\",\n",
    "        \"dataset_in_ram\": True,     # true:load dataset in ram, false: get each elem from dataset file\n",
    "        \"num_train_samples\": 15000,     # Num of pairs to consider for train\n",
    "        \"num_test_samples\": 5000,       # Num of pairs to consider for test\n",
    "        \"batch_size\": 32,           # Size of batch\n",
    "        \"test_batch_size\": 16,      # Size of batch\n",
    "        \"num_workers\": 0,           # parallel workers on dataloader, 0 if windows\n",
    "\n",
    "        #model\n",
    "        \"architecture\": \"DGCNN\",\n",
    "        \"num_points\": 100,          # num of points to use\n",
    "        \"dropout\": 0.5,             # initial dropout rate\n",
    "        \"emb_dims\": 1024,           # Dimension of embeddings\n",
    "        \"k\": 20,                    # Num of nearest neighbors to use\n",
    "        \"final_dim_DGCNN\": 64,      #dimension of final linear layer of DGCNN, max=256\n",
    "        \"dim_classification\": 64,   #dim of mid final classification layer, max= final_dim_DGCNN*2\n",
    "                                    #if 0 no middle layer\n",
    "\n",
    "        #training\n",
    "        \"epochs\": 50,               # number of episode to train\n",
    "        \"use_sgd\": False,           # Use SGD\n",
    "        \"lr\": 0.001,                # learning rate (default: 0.001, 0.1 if using sgd)\n",
    "        \"momentum\": 0.9,            # SGD momentum (default: 0.9)\n",
    "        \"scheduler\": \"cos\",         # Scheduler to use, [cos, step]\n",
    "        \"seed\": 1,                  # random seed (default: 1)\n",
    "        \"eval\": False,              # evaluate the model\n",
    "\n",
    "    }\n",
    "    \n",
    "    _init_()\n",
    "\n",
    "    io = IOStream('outputs/' + args[\"exp_name\"] + '/run.log')\n",
    "    io.cprint(str(args))\n",
    "    \n",
    "    #set up GPU\n",
    "    args[\"cuda\"] = torch.cuda.is_available()\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "    if args[\"cuda\"]:\n",
    "        io.cprint(\n",
    "            'Using GPU : ' + str(torch.cuda.current_device()) + ' from ' + str(torch.cuda.device_count()) + ' devices')\n",
    "        torch.cuda.manual_seed(args[\"seed\"])\n",
    "    else:\n",
    "        io.cprint('Using CPU')\n",
    "\n",
    "    if not args[\"eval\"]:\n",
    "        train(args, io)\n",
    "    else:\n",
    "        test(args, io)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "691f1b564724e08df0ae28181cdfea598ff9582372b176e10f27b62d8e6609e2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
