{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import h5py\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "from torch.nn import Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print grid in readable format\n",
    "def pretty_print(grid):\n",
    "    for row in grid:\n",
    "        print(row)\n",
    "\n",
    "#from path of description file get the coupling grid, model data and \n",
    "#remove models with error\n",
    "def process_file(path):\n",
    "    with open(path,'r') as f:\n",
    "        #don't consider first two lines\n",
    "        f.readline()           #segmenti x x x\n",
    "        f.readline()           #rotazioni x x x\n",
    "\n",
    "        line=f.readline()      #numero pezzi x  \n",
    "\n",
    "        #get number of fragments\n",
    "        number=int(re.findall(\"\\d+\",line)[0])\n",
    "\n",
    "        #coupling matrix\n",
    "        grid=[]\n",
    "        for i in range(number):\n",
    "            line=f.readline()\n",
    "            ret=re.findall('-1|0|1',line)\n",
    "            grid.append(list(map(int,ret)))\n",
    "\n",
    "        #model data\n",
    "        model_names=[]\n",
    "        non_valid_indeces=[]\n",
    "        for m in range(number):\n",
    "            f.readline()        #blank line\n",
    "            name=f.readline()   #model name\n",
    "            mesh=f.readline()   #mesh n\n",
    "            f.readline()        #external n\n",
    "            f.readline()        #internal n\n",
    "\n",
    "            #if mesh=0 I don't consider the element\n",
    "            mesh_n=int(mesh.rstrip().split(\" \")[1]) \n",
    "            if mesh_n!=0: \n",
    "                #try create file name\n",
    "                file_name= name.rstrip().replace(\".\",\"_\")\n",
    "                model_names.append(file_name)          \n",
    "            else:\n",
    "                #saving indices to remove later\n",
    "                non_valid_indeces.append(m)\n",
    "\n",
    "        #removing elements from grid\n",
    "        #sorted in reverse to avoid wrong index\n",
    "        for index in sorted(non_valid_indeces, reverse=True):\n",
    "            #remove row\n",
    "            del grid[index]\n",
    "            #remove columns\n",
    "            for row in grid:\n",
    "                del row[index]\n",
    "        \n",
    "        return grid,model_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a set of fragments (i.e. a subfolder)\n",
    "def get_set(folder,verbose=True):\n",
    "    folder_path=os.path.join(main_path,folder)\n",
    "    models_file=[]\n",
    "\n",
    "    #files are either text files or models\n",
    "    for file in os.listdir(folder_path):\n",
    "        if '.txt' in file:\n",
    "            description_file=file\n",
    "        elif '.stl' in file:\n",
    "            #not used, they are not in the same order of the file\n",
    "            models_file.append(file)\n",
    "\n",
    "    #parsing description file\n",
    "    full_description_file=os.path.join(folder_path,description_file)\n",
    "    grid,model_names=process_file(full_description_file)\n",
    "\n",
    "    #get path of models of current set\n",
    "    model_prefix=folder.replace(\"generatedTest_\",\"\")\n",
    "    complete_models_path=[]\n",
    "    for i in range(len(model_names)):\n",
    "        #e.g. 2021_11_29_10_00_40_Cube_001.stl\n",
    "        correct_model_name=f\"{model_prefix}_{model_names[i]}.stl\"\n",
    "        #saving only names, could be useful but now not used\n",
    "        model_names[i]=correct_model_name\n",
    "\n",
    "        complete_models_path.append(os.path.join(folder_path,correct_model_name))\n",
    "\n",
    "    #create set and put into list of sets\n",
    "    set={\"models\":complete_models_path,\"grid\":grid}\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"A set: \\n\")\n",
    "        print(\"Models: \",set[\"models\"])\n",
    "        print(\"Grid: \")\n",
    "        pretty_print(set[\"grid\"])\n",
    "\n",
    "    return set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n"
     ]
    }
   ],
   "source": [
    "#Create all the possible pairs of fragments\n",
    "#duplicates are not considered, e.g.: (i,j) - (j,i)\n",
    "def create_pairs(num):\n",
    "    lista = []\n",
    "    for i in range(num):\n",
    "        for j in range(i+1, num):\n",
    "            lista.append((i, j))\n",
    "\n",
    "    return lista\n",
    "\n",
    "print(create_pairs(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dataset_pairs(main_path,max_elements=-1,alpha=1):\n",
    "    num_total_pairs=0\n",
    "    \n",
    "    pairs_at_folder=[]  # save number of total pairs at the end of each folder, needed to split in train-test\n",
    "    #get total number of pairs we consider:\n",
    "    for folder in os.listdir(main_path):\n",
    "        #check only folders, not files\n",
    "        if '.' not in folder:\n",
    "            fragment_set=get_set(folder,verbose=False)  #set of fragments of one model\n",
    "\n",
    "            #get total number of adjacent pairs of fragments\n",
    "            #i.e. number of 1 in the grid\n",
    "            grid = np.array(fragment_set[\"grid\"])\n",
    "            unique, counts = np.unique(grid, return_counts=True)\n",
    "            dic = dict(zip(unique, counts))\n",
    "            #divide by two because we consider half of the pairs, not both of (i,j),(j,i)\n",
    "            num_zeros = int(dic[0]/2)\n",
    "            num_ones = int(dic[1]/2)\n",
    "\n",
    "            #considerare solo alpha*N coppie non adiacenti\n",
    "            max_not_adjacent_pairs=alpha*num_ones\n",
    "            estimated_num_pairs=int(min(max_not_adjacent_pairs,num_zeros)+num_ones)\n",
    "            num_total_pairs+=estimated_num_pairs\n",
    "            pairs_at_folder.append(num_total_pairs)\n",
    "\n",
    "    if max_elements>0:\n",
    "        num_total_pairs=min(max_elements,num_total_pairs)\n",
    "    print(\"dataset will contain:\", num_total_pairs,\" total pairs\")\n",
    "    return num_total_pairs,pairs_at_folder\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_pointcloud(mesh,num_points):\n",
    "    pointcloud=mesh.sample_points_poisson_disk(num_points)\n",
    "\n",
    "    center=pointcloud.get_center()\n",
    "    pointcloud=pointcloud.translate(center*-1)\n",
    "\n",
    "    return pointcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_normal(point,mesh):\n",
    "    min_index=None\n",
    "    min_dist=float(\"inf\")\n",
    "    for i in range(len(mesh.vertices)):\n",
    "        v=mesh.vertices[i]\n",
    "        squared_dist = np.sum((point-v)**2, axis=0)\n",
    "        dist = np.sqrt(squared_dist)\n",
    "        if dist<min_dist:\n",
    "            min_dist=dist\n",
    "            min_index=i\n",
    "    return mesh.vertex_normals[i]\n",
    "\n",
    "def get_nearest_normals(pointcloud,mesh):\n",
    "    normals=[]\n",
    "    for p in pointcloud.points:\n",
    "        normals.append(get_nearest_normal(p,mesh))\n",
    "    return normals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(main_path,num_points,max_elements=-1,alpha=1):\n",
    "    \n",
    "    num_elements=0\n",
    "    num_pairs=0             # number of pairs we put in dataset (train+test)\n",
    "    num_train_pairs=0       # number of pairs in train\n",
    "    num_test_pairs=0        # number of pairs in test\n",
    "    num_val_pairs=0\n",
    "    num_total_pairs=0       # total number of pairs, including those we don't put in dataset (~1.5 Millions)\n",
    "    folder_index = 0\n",
    "    tot_num_adjacent=0\n",
    "\n",
    "    sets=[]\n",
    "\n",
    "    \n",
    "    prob_test=0.15\n",
    "    prob_val=0.15\n",
    "    prob_train=1-prob_test-prob_val\n",
    "\n",
    "    print(\"Creating dataset...\")\n",
    "    dataset_total_pairs,pairs_at_folder=count_dataset_pairs(main_path,max_elements,alpha)\n",
    "\n",
    "    dataset_file=None\n",
    "    dataset_file_name=f\"dataset_{dataset_total_pairs}pairs_{num_points}points_center_random_normals_chunk.hdf5\"\n",
    "\n",
    "    with open(\".gitignore\",\"a\") as f:\n",
    "        f.write(f\"{dataset_file_name}\\n\")\n",
    "\n",
    "    try: \n",
    "        dataset_file.close()\n",
    "        os.remove(dataset_file_name)\n",
    "    except:\n",
    "        print(\"file dataset not found, creating\")\n",
    "\n",
    "    chunks_size=100\n",
    "    dataset_file=h5py.File(dataset_file_name, 'w')\n",
    "\n",
    "    train_total_pairs=int(dataset_total_pairs*(prob_train))\n",
    "    train_data=dataset_file.create_dataset(\"train_data\", (train_total_pairs,2,num_points,3),chunks=(chunks_size,2,num_points,3))\n",
    "    train_normals=dataset_file.create_dataset(\"train_normals\", (train_total_pairs,2,num_points,3),chunks=(chunks_size,2,num_points,3))\n",
    "    train_labels=dataset_file.create_dataset(\"train_labels\", (train_total_pairs,),dtype='i',chunks=(chunks_size,))\n",
    "    \n",
    "    test_total_pairs=int(dataset_total_pairs*prob_test)\n",
    "    test_data=dataset_file.create_dataset(\"test_data\", (test_total_pairs,2,num_points,3),chunks=(chunks_size,2,num_points,3))\n",
    "    test_normals=dataset_file.create_dataset(\"test_normals\", (test_total_pairs,2,num_points,3),chunks=(chunks_size,2,num_points,3))\n",
    "    test_labels=dataset_file.create_dataset(\"test_labels\", (test_total_pairs,),dtype='i',chunks=(chunks_size,))\n",
    "\n",
    "    val_total_pairs=int(dataset_total_pairs*prob_test)\n",
    "    val_data=dataset_file.create_dataset(\"val_data\", (val_total_pairs,2,num_points,3),chunks=(chunks_size,2,num_points,3))\n",
    "    val_normals=dataset_file.create_dataset(\"val_normals\", (val_total_pairs,2,num_points,3),chunks=(chunks_size,2,num_points,3))\n",
    "    val_labels=dataset_file.create_dataset(\"val_labels\", (val_total_pairs,),dtype='i',chunks=(chunks_size,))\n",
    "    \n",
    "\n",
    "    total_dataset_start_time=time.time()\n",
    "\n",
    "    #indexes for saving data on file\n",
    "    #start from -1 because dataset start at index 0\n",
    "    train_index=-1\n",
    "    test_index=-1\n",
    "    val_index=-1\n",
    "\n",
    "    print(f\"Each point cloud is sampled with {num_points} points\\n\\n\")\n",
    "    for folder in os.listdir(main_path):\n",
    "        #check only folders, not files\n",
    "        if '.' not in folder:\n",
    "            set_start_time=time.time()\n",
    "            print(f\"\\n\\nStarting set {folder_index}: folder - {folder}\")  \n",
    "\n",
    "            set_train_data=[]\n",
    "            set_train_labels=[]\n",
    "            set_train_normals=[]\n",
    "\n",
    "            set_test_data=[]\n",
    "            set_test_labels=[]\n",
    "            set_test_normals=[]\n",
    "\n",
    "            set_val_data=[]\n",
    "            set_val_labels=[]\n",
    "            set_val_normals=[]\n",
    "\n",
    "            #GET SET INFORMATIONS\n",
    "            fragment_set=get_set(folder,verbose=False)  #set of fragments of one model\n",
    "            num_elements+=len(fragment_set[\"models\"])\n",
    "            sets.append(fragment_set)\n",
    "            \n",
    "            #save pointcloud of meshes\n",
    "            set_pointcloud=[]\n",
    "            set_normals=[]\n",
    "\n",
    "            for path in fragment_set[\"models\"]:\n",
    "                mesh=o3d.io.read_triangle_mesh(path)\n",
    "                pointcloud=get_pointcloud(mesh,num_points)\n",
    "                #normals=get_nearest_normals(pointcloud,mesh)\n",
    "\n",
    "                set_pointcloud.append(pointcloud)\n",
    "                #set_normals.append(normals)\n",
    "                \n",
    "\n",
    "            #get indeces of pairs\n",
    "            pairs=create_pairs(len(fragment_set[\"models\"]))\n",
    "\n",
    "            #shuffle to get random pairs not in order\n",
    "            random.shuffle(pairs)\n",
    "\n",
    "            #get total number of adjacent pairs of fragments\n",
    "            #i.e. number of 1 in the grid\n",
    "            grid = np.array(fragment_set[\"grid\"])\n",
    "            unique, counts = np.unique(grid, return_counts=True)\n",
    "            dic = dict(zip(unique, counts))\n",
    "            #divide by two because we consider half of the pairs, not both of (i,j),(j,i)\n",
    "            num_zeros = int(dic[0]/2)\n",
    "            num_ones = int(dic[1]/2)\n",
    "\n",
    "            #considerare solo a*N coppie non adiacenti\n",
    "            max_not_adjacent_pairs=alpha*num_ones\n",
    "\n",
    "            estimated_num_pairs=int(min(max_not_adjacent_pairs,num_zeros)+num_ones)\n",
    "\n",
    "            print(\"Set stats: \")\n",
    "            print(f\"  --number of fragments: {len(fragment_set['models'])}\")\n",
    "            print(f\"  --total adjacent pairs: {num_ones}; total not adjacent pairs: {num_zeros}\")\n",
    "            print(f\"  --dataset for set will contain: {estimated_num_pairs} pairs\")\n",
    "            print(f\"     --> adj: {num_ones}    n-adj: {int(min(max_not_adjacent_pairs,num_zeros))}\")\n",
    "\n",
    "            #some stats\n",
    "            set_num_not_adjacent=0\n",
    "            set_num_adjacent=0\n",
    "            current_set_pairs=0\n",
    "            current_set_not_adj=0\n",
    "            \n",
    "            for pair in pairs:\n",
    "                #if limit of maximum pairs is not exceeded\n",
    "                if max_elements<0 or num_pairs<=max_elements:\n",
    "                        \n",
    "                    num_total_pairs+=1\n",
    "                    idx1,idx2=pair\n",
    "                    label=fragment_set[\"grid\"][idx1][idx2]\n",
    "                    \n",
    "                    #total number of not adjacent\n",
    "                    if label==0:\n",
    "                        set_num_not_adjacent+=1\n",
    "                    else:\n",
    "                        set_num_adjacent+=1\n",
    "                        tot_num_adjacent+=1\n",
    "                    \n",
    "                    #if adjacent pair or not adjacent pairs limit not exceeded\n",
    "                    if label==1 or (label==0 and set_num_not_adjacent<=max_not_adjacent_pairs):\n",
    "\n",
    "                        #add stats for number of fragments in dataset and current set\n",
    "                        num_pairs+=1\n",
    "                        current_set_pairs+=1\n",
    "\n",
    "                        #add stats for adjacent fragment in dataset\n",
    "                        if label==0: current_set_not_adj+=1\n",
    "\n",
    "                        pointcloud1=set_pointcloud[idx1]\n",
    "                        pointcloud2=set_pointcloud[idx2]\n",
    "\n",
    "                        #generate pair of pointclouds\n",
    "                        pointcloud_pair=[ np.asarray(pointcloud1.points) ,  np.asarray(pointcloud2.points) ]\n",
    "\n",
    "                        normals_pair=[ np.asarray(pointcloud1.normals) ,  np.asarray(pointcloud2.normals) ]\n",
    "\n",
    "                        #compute where to put this pair\n",
    "                        destination_set=np.random.choice([\"train\",\"test\",\"val\"], p=[prob_train, prob_test, prob_val])\n",
    "\n",
    "                        #check if still available space\n",
    "                        if destination_set==\"train\" and num_train_pairs>train_total_pairs:\n",
    "                            destination_set=\"test\"\n",
    "                        \n",
    "                        if destination_set==\"test\" and num_test_pairs>test_total_pairs:\n",
    "                            destination_set=\"val\"\n",
    "\n",
    "                        if destination_set==\"val\" and num_val_pairs>val_total_pairs:\n",
    "                            destination_set=\"\"\n",
    "\n",
    "                        if destination_set:\n",
    "                            if destination_set==\"train\":\n",
    "                                num_train_pairs+=1\n",
    "                                set_train_data.append(pointcloud_pair)\n",
    "                                set_train_labels.append(label)\n",
    "                                set_train_normals.append(normals_pair)\n",
    "\n",
    "                            elif destination_set==\"test\":\n",
    "                                num_test_pairs+=1\n",
    "                                set_test_data.append(pointcloud_pair)\n",
    "                                set_test_labels.append(label)\n",
    "                                set_test_normals.append(normals_pair)\n",
    "                                \n",
    "                            elif destination_set==\"val\":\n",
    "                                num_val_pairs+=1\n",
    "                                set_val_data.append(pointcloud_pair)\n",
    "                                set_val_labels.append(label)\n",
    "                                set_val_normals.append(normals_pair)\n",
    "\n",
    "\n",
    "            set_elapsed_time=time.time()-set_start_time\n",
    "            print(\"Set completed in: %.3f seconds\" % (set_elapsed_time),end=\": \")\n",
    "            print(f\"added {current_set_pairs} pairs --> adj: {set_num_adjacent}   n_adj: {current_set_not_adj}\")\n",
    "            print(f\"currently dataset contains {num_pairs} pairs\",end=\". \")\n",
    "            print()\n",
    "            \n",
    "            len_train=len(set_train_data)\n",
    "            train_data[train_index:train_index+len_train]=set_train_data\n",
    "            train_labels[train_index:train_index+len_train]=set_train_labels\n",
    "            train_normals[train_index:train_index+len_train]=set_train_normals\n",
    "            train_index+=len_train\n",
    "\n",
    "            len_test=len(set_test_data)\n",
    "            test_data[test_index:test_index+len_test]=set_test_data\n",
    "            test_labels[test_index:test_index+len_test]=set_test_labels\n",
    "            test_normals[test_index:test_index+len_test]=set_test_normals\n",
    "            test_index+=len_test\n",
    "\n",
    "            len_val=len(set_val_data)\n",
    "            val_data[val_index:val_index+len_val]=set_val_data\n",
    "            val_labels[val_index:val_index+len_val]=set_val_labels\n",
    "            val_normals[val_index:val_index+len_val]=set_val_normals\n",
    "            val_index+=len_val\n",
    "\n",
    "\n",
    "\n",
    "            folder_index+=1\n",
    "\n",
    "\n",
    "    dataset_file.close()\n",
    "    total_dataset_time=time.time()-total_dataset_start_time\n",
    "    print(f\"Dataset contains {num_elements} fragments --> {num_total_pairs} total pairs \")\n",
    "    print(\"we consider only: \",num_pairs,\" pairs, of which \", tot_num_adjacent, \"are adjacent\")\n",
    "    print(f\"Train set: {num_train_pairs},   test set: {num_test_pairs}, val set: {num_val_pairs}\")\n",
    "    print(f\"but it should be: train,test,val: {train_total_pairs},{test_total_pairs},{val_total_pairs}\")\n",
    "    print(\"total time: %.3f seconds\" % (total_dataset_time))\n",
    "    #return np.array(all_data),np.array(all_labels)\n",
    "\n",
    "\n",
    "\n",
    "#used in class ModelNet40\n",
    "#for now we don't use it\n",
    "def translate_pointcloud(pointcloud):\n",
    "    xyz1 = np.random.uniform(low=2./3., high=3./2., size=[3])\n",
    "    xyz2 = np.random.uniform(low=-0.2, high=0.2, size=[3])\n",
    "       \n",
    "    translated_pointcloud = np.add(np.multiply(pointcloud, xyz1), xyz2).astype('float32')\n",
    "    return translated_pointcloud         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here i save all the sets of fragments\n",
    "sets=[]\n",
    "\n",
    "#root folder\n",
    "main_path=\"produzione_29112021\"\n",
    "\n",
    "num_points=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset...\n",
      "dataset will contain: 181455  total pairs\n",
      "file dataset not found, creating\n",
      "Each point cloud is sampled with 250 points\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Starting set 0: folder - generatedTest_2021_11_29_10_00_40\n",
      "Set stats: \n",
      "  --number of fragments: 8\n",
      "  --total adjacent pairs: 19; total not adjacent pairs: 9\n",
      "  --dataset for set will contain: 28 pairs\n",
      "     --> adj: 19    n-adj: 9\n",
      "Set completed in: 0.807 seconds: added 28 pairs --> adj: 19   n_adj: 9\n",
      "currently dataset contains 28 pairs. \n",
      "\n",
      "\n",
      "Starting set 1: folder - generatedTest_2021_11_29_10_01_59\n",
      "Set stats: \n",
      "  --number of fragments: 8\n",
      "  --total adjacent pairs: 19; total not adjacent pairs: 9\n",
      "  --dataset for set will contain: 28 pairs\n",
      "     --> adj: 19    n-adj: 9\n",
      "Set completed in: 0.756 seconds: added 28 pairs --> adj: 19   n_adj: 9\n",
      "currently dataset contains 56 pairs. \n",
      "\n",
      "\n",
      "Starting set 2: folder - generatedTest_2021_11_29_10_02_12\n",
      "Set stats: \n",
      "  --number of fragments: 8\n",
      "  --total adjacent pairs: 18; total not adjacent pairs: 10\n",
      "  --dataset for set will contain: 28 pairs\n",
      "     --> adj: 18    n-adj: 10\n",
      "Set completed in: 0.982 seconds: added 28 pairs --> adj: 18   n_adj: 10\n",
      "currently dataset contains 84 pairs. \n",
      "\n",
      "\n",
      "Starting set 3: folder - generatedTest_2021_11_29_10_02_50\n",
      "Set stats: \n",
      "  --number of fragments: 27\n",
      "  --total adjacent pairs: 97; total not adjacent pairs: 254\n",
      "  --dataset for set will contain: 351 pairs\n",
      "     --> adj: 97    n-adj: 254\n",
      "Set completed in: 2.624 seconds: added 351 pairs --> adj: 97   n_adj: 254\n",
      "currently dataset contains 435 pairs. \n",
      "\n",
      "\n",
      "Starting set 4: folder - generatedTest_2021_11_29_10_02_58\n",
      "Set stats: \n",
      "  --number of fragments: 26\n",
      "  --total adjacent pairs: 86; total not adjacent pairs: 239\n",
      "  --dataset for set will contain: 325 pairs\n",
      "     --> adj: 86    n-adj: 239\n",
      "Set completed in: 2.637 seconds: added 325 pairs --> adj: 86   n_adj: 239\n",
      "currently dataset contains 760 pairs. \n",
      "\n",
      "\n",
      "Starting set 5: folder - generatedTest_2021_11_29_10_03_27\n",
      "Set stats: \n",
      "  --number of fragments: 27\n",
      "  --total adjacent pairs: 94; total not adjacent pairs: 257\n",
      "  --dataset for set will contain: 351 pairs\n",
      "     --> adj: 94    n-adj: 257\n",
      "Set completed in: 3.018 seconds: added 351 pairs --> adj: 94   n_adj: 257\n",
      "currently dataset contains 1111 pairs. \n",
      "\n",
      "\n",
      "Starting set 6: folder - generatedTest_2021_11_29_10_03_51\n",
      "Set stats: \n",
      "  --number of fragments: 64\n",
      "  --total adjacent pairs: 276; total not adjacent pairs: 1740\n",
      "  --dataset for set will contain: 1104 pairs\n",
      "     --> adj: 276    n-adj: 828\n",
      "Set completed in: 6.742 seconds: added 1104 pairs --> adj: 276   n_adj: 828\n",
      "currently dataset contains 2215 pairs. \n",
      "\n",
      "\n",
      "Starting set 7: folder - generatedTest_2021_11_29_10_04_09\n",
      "Set stats: \n",
      "  --number of fragments: 57\n",
      "  --total adjacent pairs: 242; total not adjacent pairs: 1354\n",
      "  --dataset for set will contain: 968 pairs\n",
      "     --> adj: 242    n-adj: 726\n",
      "Set completed in: 5.528 seconds: added 968 pairs --> adj: 242   n_adj: 726\n",
      "currently dataset contains 3183 pairs. \n",
      "\n",
      "\n",
      "Starting set 8: folder - generatedTest_2021_11_29_10_04_25\n",
      "Set stats: \n",
      "  --number of fragments: 59\n",
      "  --total adjacent pairs: 241; total not adjacent pairs: 1470\n",
      "  --dataset for set will contain: 964 pairs\n",
      "     --> adj: 241    n-adj: 723\n",
      "Set completed in: 6.659 seconds: added 964 pairs --> adj: 241   n_adj: 723\n",
      "currently dataset contains 4147 pairs. \n",
      "\n",
      "\n",
      "Starting set 9: folder - generatedTest_2021_11_29_10_05_19\n",
      "Set stats: \n",
      "  --number of fragments: 216\n",
      "  --total adjacent pairs: 1072; total not adjacent pairs: 22148\n",
      "  --dataset for set will contain: 4288 pairs\n",
      "     --> adj: 1072    n-adj: 3216\n",
      "Set completed in: 30.502 seconds: added 4288 pairs --> adj: 1072   n_adj: 3216\n",
      "currently dataset contains 8435 pairs. \n",
      "\n",
      "\n",
      "Starting set 10: folder - generatedTest_2021_11_29_10_06_11\n",
      "Set stats: \n",
      "  --number of fragments: 187\n",
      "  --total adjacent pairs: 930; total not adjacent pairs: 16461\n",
      "  --dataset for set will contain: 3720 pairs\n",
      "     --> adj: 930    n-adj: 2790\n",
      "Set completed in: 20.607 seconds: added 3720 pairs --> adj: 930   n_adj: 2790\n",
      "currently dataset contains 12155 pairs. \n",
      "\n",
      "\n",
      "Starting set 11: folder - generatedTest_2021_11_29_10_07_02\n",
      "Set stats: \n",
      "  --number of fragments: 208\n",
      "  --total adjacent pairs: 1019; total not adjacent pairs: 20509\n",
      "  --dataset for set will contain: 4076 pairs\n",
      "     --> adj: 1019    n-adj: 3057\n",
      "Set completed in: 24.864 seconds: added 4076 pairs --> adj: 1019   n_adj: 3057\n",
      "currently dataset contains 16231 pairs. \n",
      "\n",
      "\n",
      "Starting set 12: folder - generatedTest_2021_11_29_10_15_14\n",
      "Set stats: \n",
      "  --number of fragments: 120\n",
      "  --total adjacent pairs: 558; total not adjacent pairs: 6582\n",
      "  --dataset for set will contain: 2232 pairs\n",
      "     --> adj: 558    n-adj: 1674\n",
      "Set completed in: 16.093 seconds: added 2232 pairs --> adj: 558   n_adj: 1674\n",
      "currently dataset contains 18463 pairs. \n",
      "\n",
      "\n",
      "Starting set 13: folder - generatedTest_2021_11_29_10_16_04\n",
      "Set stats: \n",
      "  --number of fragments: 115\n",
      "  --total adjacent pairs: 532; total not adjacent pairs: 6023\n",
      "  --dataset for set will contain: 2128 pairs\n",
      "     --> adj: 532    n-adj: 1596\n",
      "Set completed in: 15.467 seconds: added 2128 pairs --> adj: 532   n_adj: 1596\n",
      "currently dataset contains 20591 pairs. \n",
      "\n",
      "\n",
      "Starting set 14: folder - generatedTest_2021_11_29_10_16_37\n",
      "Set stats: \n",
      "  --number of fragments: 119\n",
      "  --total adjacent pairs: 554; total not adjacent pairs: 6467\n",
      "  --dataset for set will contain: 2216 pairs\n",
      "     --> adj: 554    n-adj: 1662\n",
      "Set completed in: 11.689 seconds: added 2216 pairs --> adj: 554   n_adj: 1662\n",
      "currently dataset contains 22807 pairs. \n",
      "\n",
      "\n",
      "Starting set 15: folder - generatedTest_2021_11_29_10_17_44\n",
      "Set stats: \n",
      "  --number of fragments: 192\n",
      "  --total adjacent pairs: 936; total not adjacent pairs: 17400\n",
      "  --dataset for set will contain: 3744 pairs\n",
      "     --> adj: 936    n-adj: 2808\n",
      "Set completed in: 26.531 seconds: added 3744 pairs --> adj: 936   n_adj: 2808\n",
      "currently dataset contains 26551 pairs. \n",
      "\n",
      "\n",
      "Starting set 16: folder - generatedTest_2021_11_29_10_19_23\n",
      "Set stats: \n",
      "  --number of fragments: 176\n",
      "  --total adjacent pairs: 842; total not adjacent pairs: 14558\n",
      "  --dataset for set will contain: 3368 pairs\n",
      "     --> adj: 842    n-adj: 2526\n",
      "Set completed in: 23.363 seconds: added 3368 pairs --> adj: 842   n_adj: 2526\n",
      "currently dataset contains 29919 pairs. \n",
      "\n",
      "\n",
      "Starting set 17: folder - generatedTest_2021_11_29_10_20_52\n",
      "Set stats: \n",
      "  --number of fragments: 167\n",
      "  --total adjacent pairs: 778; total not adjacent pairs: 13083\n",
      "  --dataset for set will contain: 3112 pairs\n",
      "     --> adj: 778    n-adj: 2334\n",
      "Set completed in: 18.225 seconds: added 3112 pairs --> adj: 778   n_adj: 2334\n",
      "currently dataset contains 33031 pairs. \n",
      "\n",
      "\n",
      "Starting set 18: folder - generatedTest_2021_11_29_10_21_45\n",
      "Set stats: \n",
      "  --number of fragments: 160\n",
      "  --total adjacent pairs: 761; total not adjacent pairs: 11959\n",
      "  --dataset for set will contain: 3044 pairs\n",
      "     --> adj: 761    n-adj: 2283\n",
      "Set completed in: 18.962 seconds: added 3044 pairs --> adj: 761   n_adj: 2283\n",
      "currently dataset contains 36075 pairs. \n",
      "\n",
      "\n",
      "Starting set 19: folder - generatedTest_2021_11_29_10_22_52\n",
      "Set stats: \n",
      "  --number of fragments: 138\n",
      "  --total adjacent pairs: 605; total not adjacent pairs: 8848\n",
      "  --dataset for set will contain: 2420 pairs\n",
      "     --> adj: 605    n-adj: 1815\n",
      "Set completed in: 15.887 seconds: added 2420 pairs --> adj: 605   n_adj: 1815\n",
      "currently dataset contains 38495 pairs. \n",
      "\n",
      "\n",
      "Starting set 20: folder - generatedTest_2021_11_29_10_23_50\n",
      "Set stats: \n",
      "  --number of fragments: 146\n",
      "  --total adjacent pairs: 684; total not adjacent pairs: 9901\n",
      "  --dataset for set will contain: 2736 pairs\n",
      "     --> adj: 684    n-adj: 2052\n",
      "Set completed in: 14.710 seconds: added 2736 pairs --> adj: 684   n_adj: 2052\n",
      "currently dataset contains 41231 pairs. \n",
      "\n",
      "\n",
      "Starting set 21: folder - generatedTest_2021_11_29_10_24_57\n",
      "Set stats: \n",
      "  --number of fragments: 216\n",
      "  --total adjacent pairs: 1068; total not adjacent pairs: 22152\n",
      "  --dataset for set will contain: 4272 pairs\n",
      "     --> adj: 1068    n-adj: 3204\n",
      "Set completed in: 28.223 seconds: added 4272 pairs --> adj: 1068   n_adj: 3204\n",
      "currently dataset contains 45503 pairs. \n",
      "\n",
      "\n",
      "Starting set 22: folder - generatedTest_2021_11_29_10_26_01\n",
      "Set stats: \n",
      "  --number of fragments: 180\n",
      "  --total adjacent pairs: 856; total not adjacent pairs: 15254\n",
      "  --dataset for set will contain: 3424 pairs\n",
      "     --> adj: 856    n-adj: 2568\n",
      "Set completed in: 19.936 seconds: added 3424 pairs --> adj: 856   n_adj: 2568\n",
      "currently dataset contains 48927 pairs. \n",
      "\n",
      "\n",
      "Starting set 23: folder - generatedTest_2021_11_29_10_26_56\n",
      "Set stats: \n",
      "  --number of fragments: 210\n",
      "  --total adjacent pairs: 997; total not adjacent pairs: 20948\n",
      "  --dataset for set will contain: 3988 pairs\n",
      "     --> adj: 997    n-adj: 2991\n",
      "Set completed in: 22.107 seconds: added 3988 pairs --> adj: 997   n_adj: 2991\n",
      "currently dataset contains 52915 pairs. \n",
      "\n",
      "\n",
      "Starting set 24: folder - generatedTest_2021_11_29_10_27_56\n",
      "Set stats: \n",
      "  --number of fragments: 84\n",
      "  --total adjacent pairs: 360; total not adjacent pairs: 3126\n",
      "  --dataset for set will contain: 1440 pairs\n",
      "     --> adj: 360    n-adj: 1080\n",
      "Set completed in: 9.277 seconds: added 1440 pairs --> adj: 360   n_adj: 1080\n",
      "currently dataset contains 54355 pairs. \n",
      "\n",
      "\n",
      "Starting set 25: folder - generatedTest_2021_11_29_10_28_22\n",
      "Set stats: \n",
      "  --number of fragments: 83\n",
      "  --total adjacent pairs: 350; total not adjacent pairs: 3053\n",
      "  --dataset for set will contain: 1400 pairs\n",
      "     --> adj: 350    n-adj: 1050\n",
      "Set completed in: 9.652 seconds: added 1400 pairs --> adj: 350   n_adj: 1050\n",
      "currently dataset contains 55755 pairs. \n",
      "\n",
      "\n",
      "Starting set 26: folder - generatedTest_2021_11_29_10_28_46\n",
      "Set stats: \n",
      "  --number of fragments: 83\n",
      "  --total adjacent pairs: 343; total not adjacent pairs: 3060\n",
      "  --dataset for set will contain: 1372 pairs\n",
      "     --> adj: 343    n-adj: 1029\n",
      "Set completed in: 8.185 seconds: added 1372 pairs --> adj: 343   n_adj: 1029\n",
      "currently dataset contains 57127 pairs. \n",
      "\n",
      "\n",
      "Starting set 27: folder - generatedTest_2021_11_29_10_29_28\n",
      "Set stats: \n",
      "  --number of fragments: 105\n",
      "  --total adjacent pairs: 470; total not adjacent pairs: 4990\n",
      "  --dataset for set will contain: 1880 pairs\n",
      "     --> adj: 470    n-adj: 1410\n",
      "Set completed in: 13.977 seconds: added 1880 pairs --> adj: 470   n_adj: 1410\n",
      "currently dataset contains 59007 pairs. \n",
      "\n",
      "\n",
      "Starting set 28: folder - generatedTest_2021_11_29_10_31_27\n",
      "Set stats: \n",
      "  --number of fragments: 99\n",
      "  --total adjacent pairs: 427; total not adjacent pairs: 4424\n",
      "  --dataset for set will contain: 1708 pairs\n",
      "     --> adj: 427    n-adj: 1281\n",
      "Set completed in: 11.583 seconds: added 1708 pairs --> adj: 427   n_adj: 1281\n",
      "currently dataset contains 60715 pairs. \n",
      "\n",
      "\n",
      "Starting set 29: folder - generatedTest_2021_11_29_10_32_18\n",
      "Set stats: \n",
      "  --number of fragments: 103\n",
      "  --total adjacent pairs: 457; total not adjacent pairs: 4796\n",
      "  --dataset for set will contain: 1828 pairs\n",
      "     --> adj: 457    n-adj: 1371\n",
      "Set completed in: 12.416 seconds: added 1828 pairs --> adj: 457   n_adj: 1371\n",
      "currently dataset contains 62543 pairs. \n",
      "\n",
      "\n",
      "Starting set 30: folder - generatedTest_2021_11_29_10_36_43\n",
      "Set stats: \n",
      "  --number of fragments: 729\n",
      "  --total adjacent pairs: 3951; total not adjacent pairs: 261405\n",
      "  --dataset for set will contain: 15804 pairs\n",
      "     --> adj: 3951    n-adj: 11853\n",
      "Set completed in: 90.185 seconds: added 15804 pairs --> adj: 3951   n_adj: 11853\n",
      "currently dataset contains 78347 pairs. \n",
      "\n",
      "\n",
      "Starting set 31: folder - generatedTest_2021_11_29_10_41_22\n",
      "Set stats: \n",
      "  --number of fragments: 695\n",
      "  --total adjacent pairs: 3662; total not adjacent pairs: 237503\n",
      "  --dataset for set will contain: 14648 pairs\n",
      "     --> adj: 3662    n-adj: 10986\n",
      "Set completed in: 81.740 seconds: added 14648 pairs --> adj: 3662   n_adj: 10986\n",
      "currently dataset contains 92995 pairs. \n",
      "\n",
      "\n",
      "Starting set 32: folder - generatedTest_2021_11_29_10_45_42\n",
      "Set stats: \n",
      "  --number of fragments: 645\n",
      "  --total adjacent pairs: 3374; total not adjacent pairs: 204316\n",
      "  --dataset for set will contain: 13496 pairs\n",
      "     --> adj: 3374    n-adj: 10122\n",
      "Set completed in: 70.863 seconds: added 13496 pairs --> adj: 3374   n_adj: 10122\n",
      "currently dataset contains 106491 pairs. \n",
      "\n",
      "\n",
      "Starting set 33: folder - generatedTest_2021_11_29_10_48_20\n",
      "Set stats: \n",
      "  --number of fragments: 343\n",
      "  --total adjacent pairs: 1801; total not adjacent pairs: 56852\n",
      "  --dataset for set will contain: 7204 pairs\n",
      "     --> adj: 1801    n-adj: 5403\n",
      "Set completed in: 46.543 seconds: added 7204 pairs --> adj: 1801   n_adj: 5403\n",
      "currently dataset contains 113695 pairs. \n",
      "\n",
      "\n",
      "Starting set 34: folder - generatedTest_2021_11_29_10_51_20\n",
      "Set stats: \n",
      "  --number of fragments: 305\n",
      "  --total adjacent pairs: 1546; total not adjacent pairs: 44814\n",
      "  --dataset for set will contain: 6184 pairs\n",
      "     --> adj: 1546    n-adj: 4638\n",
      "Set completed in: 40.077 seconds: added 6184 pairs --> adj: 1546   n_adj: 4638\n",
      "currently dataset contains 119879 pairs. \n",
      "\n",
      "\n",
      "Starting set 35: folder - generatedTest_2021_11_29_10_53_31\n",
      "Set stats: \n",
      "  --number of fragments: 314\n",
      "  --total adjacent pairs: 1587; total not adjacent pairs: 47554\n",
      "  --dataset for set will contain: 6348 pairs\n",
      "     --> adj: 1587    n-adj: 4761\n",
      "Set completed in: 33.253 seconds: added 6348 pairs --> adj: 1587   n_adj: 4761\n",
      "currently dataset contains 126227 pairs. \n",
      "\n",
      "\n",
      "Starting set 36: folder - generatedTest_2021_11_29_10_54_15\n",
      "Set stats: \n",
      "  --number of fragments: 140\n",
      "  --total adjacent pairs: 666; total not adjacent pairs: 9064\n",
      "  --dataset for set will contain: 2664 pairs\n",
      "     --> adj: 666    n-adj: 1998\n",
      "Set completed in: 17.817 seconds: added 2664 pairs --> adj: 666   n_adj: 1998\n",
      "currently dataset contains 128891 pairs. \n",
      "\n",
      "\n",
      "Starting set 37: folder - generatedTest_2021_11_29_10_55_01\n",
      "Set stats: \n",
      "  --number of fragments: 135\n",
      "  --total adjacent pairs: 626; total not adjacent pairs: 8419\n",
      "  --dataset for set will contain: 2504 pairs\n",
      "     --> adj: 626    n-adj: 1878\n",
      "Set completed in: 14.244 seconds: added 2504 pairs --> adj: 626   n_adj: 1878\n",
      "currently dataset contains 131395 pairs. \n",
      "\n",
      "\n",
      "Starting set 38: folder - generatedTest_2021_11_29_10_55_44\n",
      "Set stats: \n",
      "  --number of fragments: 128\n",
      "  --total adjacent pairs: 602; total not adjacent pairs: 7526\n",
      "  --dataset for set will contain: 2408 pairs\n",
      "     --> adj: 602    n-adj: 1806\n",
      "Set completed in: 12.643 seconds: added 2408 pairs --> adj: 602   n_adj: 1806\n",
      "currently dataset contains 133803 pairs. \n",
      "\n",
      "\n",
      "Starting set 39: folder - generatedTest_2021_11_29_10_57_36\n",
      "Set stats: \n",
      "  --number of fragments: 175\n",
      "  --total adjacent pairs: 854; total not adjacent pairs: 14371\n",
      "  --dataset for set will contain: 3416 pairs\n",
      "     --> adj: 854    n-adj: 2562\n",
      "Set completed in: 18.462 seconds: added 3416 pairs --> adj: 854   n_adj: 2562\n",
      "currently dataset contains 137219 pairs. \n",
      "\n",
      "\n",
      "Starting set 40: folder - generatedTest_2021_11_29_10_58_19\n",
      "Set stats: \n",
      "  --number of fragments: 135\n",
      "  --total adjacent pairs: 615; total not adjacent pairs: 8430\n",
      "  --dataset for set will contain: 2460 pairs\n",
      "     --> adj: 615    n-adj: 1845\n",
      "Set completed in: 15.010 seconds: added 2460 pairs --> adj: 615   n_adj: 1845\n",
      "currently dataset contains 139679 pairs. \n",
      "\n",
      "\n",
      "Starting set 41: folder - generatedTest_2021_11_29_10_59_06\n",
      "Set stats: \n",
      "  --number of fragments: 162\n",
      "  --total adjacent pairs: 765; total not adjacent pairs: 12276\n",
      "  --dataset for set will contain: 3060 pairs\n",
      "     --> adj: 765    n-adj: 2295\n",
      "Set completed in: 21.407 seconds: added 3060 pairs --> adj: 765   n_adj: 2295\n",
      "currently dataset contains 142739 pairs. \n",
      "\n",
      "\n",
      "Starting set 42: folder - generatedTest_2021_11_29_11_02_28\n",
      "Set stats: \n",
      "  --number of fragments: 504\n",
      "  --total adjacent pairs: 2695; total not adjacent pairs: 124061\n",
      "  --dataset for set will contain: 10780 pairs\n",
      "     --> adj: 2695    n-adj: 8085\n",
      "Set completed in: 61.317 seconds: added 10780 pairs --> adj: 2695   n_adj: 8085\n",
      "currently dataset contains 153519 pairs. \n",
      "\n",
      "\n",
      "Starting set 43: folder - generatedTest_2021_11_29_11_05_39\n",
      "Set stats: \n",
      "  --number of fragments: 452\n",
      "  --total adjacent pairs: 2316; total not adjacent pairs: 99610\n",
      "  --dataset for set will contain: 9264 pairs\n",
      "     --> adj: 2316    n-adj: 6948\n",
      "Set completed in: 57.873 seconds: added 9264 pairs --> adj: 2316   n_adj: 6948\n",
      "currently dataset contains 162783 pairs. \n",
      "\n",
      "\n",
      "Starting set 44: folder - generatedTest_2021_11_29_11_27_51\n",
      "Set stats: \n",
      "  --number of fragments: 472\n",
      "  --total adjacent pairs: 2426; total not adjacent pairs: 108730\n",
      "  --dataset for set will contain: 9704 pairs\n",
      "     --> adj: 2426    n-adj: 7278\n",
      "Set completed in: 53.723 seconds: added 9704 pairs --> adj: 2426   n_adj: 7278\n",
      "currently dataset contains 172487 pairs. \n",
      "\n",
      "\n",
      "Starting set 45: folder - generatedTest_2021_11_29_11_30_35\n",
      "Set stats: \n",
      "  --number of fragments: 168\n",
      "  --total adjacent pairs: 796; total not adjacent pairs: 13232\n",
      "  --dataset for set will contain: 3184 pairs\n",
      "     --> adj: 796    n-adj: 2388\n",
      "Set completed in: 18.028 seconds: added 3184 pairs --> adj: 796   n_adj: 2388\n",
      "currently dataset contains 175671 pairs. \n",
      "\n",
      "\n",
      "Starting set 46: folder - generatedTest_2021_11_29_11_32_07\n",
      "Set stats: \n",
      "  --number of fragments: 154\n",
      "  --total adjacent pairs: 693; total not adjacent pairs: 11088\n",
      "  --dataset for set will contain: 2772 pairs\n",
      "     --> adj: 693    n-adj: 2079\n",
      "Set completed in: 19.078 seconds: added 2772 pairs --> adj: 693   n_adj: 2079\n",
      "currently dataset contains 178443 pairs. \n",
      "\n",
      "\n",
      "Starting set 47: folder - generatedTest_2021_11_29_11_36_31\n",
      "Set stats: \n",
      "  --number of fragments: 163\n",
      "  --total adjacent pairs: 753; total not adjacent pairs: 12450\n",
      "  --dataset for set will contain: 3012 pairs\n",
      "     --> adj: 753    n-adj: 2259\n",
      "Set completed in: 17.750 seconds: added 3012 pairs --> adj: 753   n_adj: 2259\n",
      "currently dataset contains 181455 pairs. \n",
      "Dataset contains 9210 fragments --> 1568204 total pairs \n",
      "we consider only:  181455  pairs, of which  45419 are adjacent\n",
      "Train set: 126593,   test set: 27219, val set: 27219\n",
      "but it should be: train,test,val: 127018,27218,27218\n",
      "total time: 1098.205 seconds\n"
     ]
    }
   ],
   "source": [
    "create_dataset(main_path,num_points,alpha=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414\n",
      "[[ 0.99411643 -0.0440013   0.09897698]\n",
      " [ 0.99411643 -0.0440013   0.09897698]\n",
      " [ 0.99411643 -0.0440013   0.09897698]\n",
      " [ 0.99329954  0.08037464  0.08304144]\n",
      " [ 0.99329954  0.08037464  0.08304144]\n",
      " [ 0.99329954  0.08037464  0.08304144]\n",
      " [ 0.99499571  0.00420776  0.09982836]\n",
      " [ 0.99499571  0.00420776  0.09982836]\n",
      " [ 0.99499571  0.00420776  0.09982836]]\n"
     ]
    }
   ],
   "source": [
    "path=r\"produzione_29112021\\generatedTest_2021_11_29_10_00_40\\2021_11_29_10_00_40_Cube_001.stl\"\n",
    "mesh=o3d.io.read_triangle_mesh(path)\n",
    "#normals=mesh.compute_vertex_normals\n",
    "print(len(np.asarray(mesh.vertices)))\n",
    "print(np.asarray(mesh.vertex_normals[:9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointcloud=get_pointcloud(mesh,500)\n",
    "#normals=get_nearest_normals(pointcloud,mesh)\n",
    "pointcloud.estimate_normals()\n",
    "#pointcloud.colors=o3d.utility.Vector3dVector(np.asarray(normals))\n",
    "\n",
    "#o3d.visualization.draw_geometries([pointcloud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99411643 -0.0440013   0.09897698]\n",
      " [ 0.99411643 -0.0440013   0.09897698]\n",
      " [ 0.99411643 -0.0440013   0.09897698]\n",
      " [ 0.99411643 -0.0440013   0.09897698]\n",
      " [ 0.99329954  0.08037464  0.08304144]\n",
      " [ 0.99329954  0.08037464  0.08304144]\n",
      " [ 0.99499571  0.00420776  0.09982836]\n",
      " [ 0.99499571  0.00420776  0.09982836]\n",
      " [ 0.99499571  0.00420776  0.09982836]\n",
      " [ 0.99499571  0.00420776  0.09982836]]\n",
      "[[ 9.85525517e-01 -1.90644985e-02  1.68451774e-01]\n",
      " [ 9.89164354e-01 -7.15395838e-03  1.46637992e-01]\n",
      " [ 9.97662392e-01  3.01908246e-02 -6.13046889e-02]\n",
      " [ 9.98406033e-01  1.88859203e-02  5.31856589e-02]\n",
      " [ 7.56180311e-01  3.13980259e-01  5.74114739e-01]\n",
      " [ 6.73168239e-01  4.25400315e-02  7.38264768e-01]\n",
      " [ 9.99825837e-01 -5.40302683e-03 -1.78634416e-02]\n",
      " [ 9.74934945e-01  6.24924227e-03 -2.22402338e-01]\n",
      " [ 9.98080595e-01  6.72552603e-03  6.15621145e-02]\n",
      " [ 9.96437196e-01 -4.31476010e-04  8.43369875e-02]]\n"
     ]
    }
   ],
   "source": [
    "pointcloud=get_pointcloud(mesh,250)\n",
    "#normals=get_nearest_normals(pointcloud,mesh)\n",
    "#pointcloud.estimate_normals()\n",
    "#pointcloud.colors=o3d.utility.Vector3dVector(np.asarray(normals))\n",
    "print(np.asarray(pointcloud.normals)[:10])\n",
    "#o3d.visualization.draw_geometries([pointcloud])\n",
    "pointcloud.estimate_normals()\n",
    "print(np.asarray(pointcloud.normals)[:10])\n",
    "#o3d.visualization.draw_geometries([pointcloud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6305d24f3e56ac2ca2871aacb4eb187216de08a2a8667a1a48c2c574de0d34f3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('EAI': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
