{"cells":[{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T15:32:00.556373Z","iopub.status.busy":"2022-05-12T15:32:00.556066Z","iopub.status.idle":"2022-05-12T15:32:00.562943Z","shell.execute_reply":"2022-05-12T15:32:00.562164Z","shell.execute_reply.started":"2022-05-12T15:32:00.556340Z"},"trusted":true},"outputs":[],"source":["from __future__ import print_function\n","import os\n","import h5py\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR,ReduceLROnPlateau\n","from torch.utils.data import Dataset,DataLoader\n","import torch.nn.init as init\n","\n","import sklearn.metrics as metrics"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T15:32:00.601716Z","iopub.status.busy":"2022-05-12T15:32:00.601162Z","iopub.status.idle":"2022-05-12T15:32:00.607614Z","shell.execute_reply":"2022-05-12T15:32:00.606674Z","shell.execute_reply.started":"2022-05-12T15:32:00.601679Z"},"trusted":true},"outputs":[],"source":["#wandb used to log metrics and results\n","import wandb\n","#insert here wandb key\n","WANDB_KEY=\"\"\n","if WANDB_KEY:\n","    wandb.login(key=WANDB_KEY)"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T15:32:00.644610Z","iopub.status.busy":"2022-05-12T15:32:00.644263Z","iopub.status.idle":"2022-05-12T15:32:00.659939Z","shell.execute_reply":"2022-05-12T15:32:00.659119Z","shell.execute_reply.started":"2022-05-12T15:32:00.644579Z"},"trusted":true},"outputs":[],"source":["#translate each pointcloud of a random value\n","def translate_pointcloud(pointcloud):\n","    xyz1 = np.random.uniform(low=2./3., high=3./2., size=[3])\n","    xyz2 = np.random.uniform(low=-0.2, high=0.2, size=[3])\n","       \n","    translated_pointcloud = np.add(np.multiply(pointcloud, xyz1), xyz2).astype('float32')\n","    return translated_pointcloud\n","\n","#create dataset with fragments\n","class MyDataset(Dataset):\n","    def __init__(self,args,extra_args,partition='train'):\n","        self.args=args\n","        self.dataset_file=h5py.File(extra_args[\"dataset_file_path\"], 'r')\n","        self.num_train=min(args[\"num_train_samples\"],len(self.dataset_file[\"train_data\"]))\n","        self.num_test=min(args[\"num_test_samples\"],len(self.dataset_file[\"test_data\"]))\n","        self.num_val=min(args[\"num_test_samples\"],len(self.dataset_file[\"val_data\"]))\n","        \n","        #get datasets\n","        self.train_set=self.dataset_file[\"train_data\"][:self.num_train]\n","        self.train_labels=self.dataset_file[\"train_labels\"][:self.num_train]\n","        self.train_normals=self.dataset_file[\"train_normals\"][:self.num_train]\n","        \n","        self.test_set=self.dataset_file[\"test_data\"][:self.num_test]\n","        self.test_normals=self.dataset_file[\"test_normals\"][:self.num_test]\n","        self.test_labels=self.dataset_file[\"test_labels\"][:self.num_test]\n","\n","        self.val_set=self.dataset_file[\"val_data\"][:self.num_val]\n","        self.val_normals=self.dataset_file[\"val_normals\"][:self.num_val]\n","        self.val_labels=self.dataset_file[\"val_labels\"][:self.num_val]\n","\n","\n","        self.partition = partition        \n","\n","    def __getitem__(self, item):\n","        if self.partition==\"train\":\n","            pointclouds_pair=self.train_set[item]\n","            normals_pair=self.train_normals[item]\n","            label=self.train_labels[item]\n","            \n","            #preprocess pointclouds\n","            pointcloud1=pointclouds_pair[0,:,:]\n","            pointcloud2=pointclouds_pair[1,:,:]\n","            pointcloud1 = translate_pointcloud(pointcloud1)\n","            pointcloud2 = translate_pointcloud(pointcloud2)\n","            pointclouds_pair=torch.Tensor([pointcloud1,pointcloud2])\n","\n","        elif self.partition==\"test\":        \n","            pointclouds_pair=self.test_set[item]\n","            normals_pair=self.test_normals[item]\n","            label=self.test_labels[item]\n","        \n","        elif self.partition==\"val\":        \n","            pointclouds_pair=self.val_set[item]\n","            normals_pair=self.val_normals[item]\n","            label=self.val_labels[item]\n","\n","        return pointclouds_pair,normals_pair, label\n","\n","    def __len__(self):\n","        if self.partition==\"train\":\n","            return self.num_train\n","        elif self.partition==\"val\":\n","            return self.num_val\n","        else:\n","            return self.num_test\n"]},{"cell_type":"markdown","metadata":{},"source":["## Util"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T15:32:00.694949Z","iopub.status.busy":"2022-05-12T15:32:00.694665Z","iopub.status.idle":"2022-05-12T15:32:00.704133Z","shell.execute_reply":"2022-05-12T15:32:00.703237Z","shell.execute_reply.started":"2022-05-12T15:32:00.694921Z"},"trusted":true},"outputs":[],"source":["\n","def cal_loss(pred, gold, smoothing=False):\n","    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n","    #we do not use label smoothing\n","\n","    gold = gold.contiguous().view(-1)\n","\n","    if smoothing:\n","        eps = 0.2\n","        n_class = pred.size(1)\n","\n","        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n","        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n","        log_prb = F.log_softmax(pred, dim=1)\n","\n","        loss = -(one_hot * log_prb).sum(dim=1).mean()\n","    else:\n","        loss = F.binary_cross_entropy(pred, gold, reduction='mean')\n","\n","    return loss\n","\n","#class to print on file some debug stuff\n","class IOStream():\n","    def __init__(self, path):\n","        self.f = open(path, 'a')\n","\n","    def cprint(self, text):\n","        print(text)\n","        self.f.write(text+'\\n')\n","        self.f.flush()\n","\n","    def close(self):\n","        self.f.close()\n"]},{"cell_type":"markdown","metadata":{},"source":["## Models"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T15:32:00.732190Z","iopub.status.busy":"2022-05-12T15:32:00.731911Z","iopub.status.idle":"2022-05-12T15:32:00.738805Z","shell.execute_reply":"2022-05-12T15:32:00.738159Z","shell.execute_reply.started":"2022-05-12T15:32:00.732162Z"},"trusted":true},"outputs":[],"source":["#zero out some points in the pointcluod and related normal\n","class PointDropout(torch.nn.Module):\n","\n","    def __init__(self, p=0.0):\n","        super().__init__()\n","        self.p = torch.nn.Parameter(torch.tensor(1 - p), requires_grad=False)\n","        self.distrib = torch.distributions.Bernoulli(self.p)\n","\n","    def forward(self, x,n):\n","        #dropout works only in training\n","        if self.training:\n","            dropout_mask = self.distrib.sample(torch.Size([x.shape[0],x.shape[2]]))\n","            for i in range(len(x)):\n","                x[i]=x[i]*dropout_mask[i]\n","                n[i]=n[i]*dropout_mask[i]\n","        return x,n"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T15:32:00.775613Z","iopub.status.busy":"2022-05-12T15:32:00.775182Z","iopub.status.idle":"2022-05-12T15:32:00.786106Z","shell.execute_reply":"2022-05-12T15:32:00.785206Z","shell.execute_reply.started":"2022-05-12T15:32:00.775584Z"},"trusted":true},"outputs":[],"source":["#k-nearest neighbour, part of edge convolution step\n","def knn(x, k):\n","    inner = -2*torch.matmul(x.transpose(2, 1), x)\n","    xx = torch.sum(x**2, dim=1, keepdim=True)\n","    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n"," \n","    idx = pairwise_distance.topk(k=k, dim=-1)[1]   # (batch_size, num_points, k)\n","    return idx\n","\n","\n","def get_graph_feature(x, k=20,device=\"cpu\", idx=None, dim9=False):\n","    batch_size = x.size(0)\n","    num_points = x.size(2)\n","\n","    x = x.view(batch_size, -1, num_points)\n","\n","    if idx is None:\n","        if dim9 == False:\n","            idx = knn(x, k=k)   # (batch_size, num_points, k)\n","        else:\n","            idx = knn(x[:, 6:], k=k)\n","    device = torch.device(device)\n","\n","    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1)*num_points\n","\n","    idx = idx + idx_base\n","\n","    idx = idx.view(-1)\n"," \n","    _, num_dims, _ = x.size()\n","\n","    x = x.transpose(2, 1).contiguous()   # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n","    feature = x.view(batch_size*num_points, -1)[idx, :]\n","    feature = feature.view(batch_size, num_points, k, num_dims) \n","    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n","    \n","    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n","  \n","    return feature      # (batch_size, 2*num_dims, num_points, k)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Model 1\n","Normals are concatenated to the output of the two DGCNNs and then 3 classification layers act as classifier"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T15:32:00.813919Z","iopub.status.busy":"2022-05-12T15:32:00.813709Z","iopub.status.idle":"2022-05-12T15:32:00.841865Z","shell.execute_reply":"2022-05-12T15:32:00.841071Z","shell.execute_reply.started":"2022-05-12T15:32:00.813893Z"},"trusted":true},"outputs":[],"source":["\n","class DGCNN_cls2(nn.Module):\n","    def __init__(self, args, output_channels=1):\n","        super(DGCNN_cls2, self).__init__()\n","        self.args = args\n","        self.device=\"cuda\" if self.args[\"cuda\"] else \"cpu\"\n","        self.k = args[\"k\"]\n","        \n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.bn3 = nn.BatchNorm2d(128)\n","        self.bn4 = nn.BatchNorm2d(256)\n","        self.bn5 = nn.BatchNorm1d(args[\"emb_dims\"])\n","\n","        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n","                                   self.bn1,\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        self.conv2 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n","                                   self.bn2,\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 128, kernel_size=1, bias=False),\n","                                   self.bn3,\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        self.conv4 = nn.Sequential(nn.Conv2d(128*2, 256, kernel_size=1, bias=False),\n","                                   self.bn4,\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        self.conv5 = nn.Sequential(nn.Conv1d(512, args[\"emb_dims\"], kernel_size=1, bias=False),\n","                                   self.bn5,\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        \n","        self.node_num=args[\"emb_dims\"]\n","        dim_normal=args[\"num_points\"]*3*2\n","        \n","        self.linear1 = nn.Linear(args[\"emb_dims\"]*2 + dim_normal//2, self.node_num, bias=False)\n","        self.bn6 = nn.BatchNorm1d(self.node_num)\n","        self.dp1 = nn.Dropout(p=args[\"dropout\"])\n","        \n","       \n","        self.dp3=nn.Dropout(p=args[\"dropout\"])\n","        \n","        dim_normal=args[\"num_points\"]*3\n","        \n","        self.linearAlternative = nn.Linear(args[\"emb_dims\"]*4 + dim_normal*2, args[\"emb_dims\"], bias=False)\n","        self.bnAlt = nn.BatchNorm1d(args[\"emb_dims\"])\n","        self.linearAlternative2 = nn.Linear(args[\"emb_dims\"], args[\"emb_dims\"]//2, bias=False)\n","        self.bnAlt2 = nn.BatchNorm1d(args[\"emb_dims\"]//2)\n","        self.linearAlternative3 = nn.Linear(args[\"emb_dims\"]//2, output_channels, bias=False)\n","       \n","       \n","    \n","    def dgcnn_step(self,x):\n","        batch_size = x.size(0)\n","        x = get_graph_feature(x, k=self.k,device=self.device)           # (batch_size, 3, num_points) -> (batch_size, 3*2, num_points, k)\n","        x = self.conv1(x)                                               # (batch_size, 3*2, num_points, k) -> (batch_size, 64, num_points, k)\n","        x1 = x.max(dim=-1, keepdim=False)[0]                            # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n","\n","        x = get_graph_feature(x1, k=self.k,device=self.device)          # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n","        x = self.conv2(x)                                               # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n","        x2 = x.max(dim=-1, keepdim=False)[0]                            # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n","\n","        x = get_graph_feature(x2, k=self.k,device=self.device)          # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n","        x = self.conv3(x)                                               # (batch_size, 64*2, num_points, k) -> (batch_size, 128, num_points, k)\n","        x3 = x.max(dim=-1, keepdim=False)[0]                            # (batch_size, 128, num_points, k) -> (batch_size, 128, num_points)\n","\n","        x = get_graph_feature(x3, k=self.k,device=self.device)          # (batch_size, 128, num_points) -> (batch_size, 128*2, num_points, k)\n","        x = self.conv4(x)                                               # (batch_size, 128*2, num_points, k) -> (batch_size, 256, num_points, k)\n","        x4 = x.max(dim=-1, keepdim=False)[0]                            # (batch_size, 256, num_points, k) -> (batch_size, 256, num_points)\n","\n","        x = torch.cat((x1, x2, x3, x4), dim=1)                          # (batch_size, 64+64+128+256, num_points)\n","\n","        x = self.conv5(x)                                               # (batch_size, 64+64+128+256, num_points) -> (batch_size, emb_dims, num_points)\n","        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)           # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims)\n","        x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)           # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims)\n","        x = torch.cat((x1, x2), 1)                                      # (batch_size, emb_dims*2)\n","        \n","        return x\n","\n","    def forward(self, x,n,invert=False):\n","       \n","        if invert:\n","            first=x[:,1,:,:]\n","            second=x[:,0,:,:]\n","            second_n=n[:,0,:,:]\n","            first_n=n[:,1,:,:]\n","        else:\n","            first=x[:,0,:,:]\n","            first_n=n[:,0,:,:]\n","            second=x[:,1,:,:]\n","            second_n=n[:,1,:,:]\n","        \n","        \n","        #compute DGCNN output from both pointclouds \n","        ret1=self.dgcnn_step(first)      \n","        ret2=self.dgcnn_step(second)\n","        \n","        normals=torch.cat((first_n,second_n),dim=1)\n","        #combine output and use classifier\n","        x=torch.cat((ret1,ret2),dim=1)\n","        #combine output and normals\n","        x=torch.cat((x,normals),dim=1)\n","        \n","\n","        #classifier\n","        x=F.leaky_relu(self.bnAlt(self.linearAlternative(x)), negative_slope=0.2)\n","        x=self.dp3(x)\n","        x=F.leaky_relu(self.bnAlt2(self.linearAlternative2(x)), negative_slope=0.2)\n","        x=self.dp3(x)\n","        x=self.linearAlternative3(x)\n","        \n","        \n","\n","        return x\n"]},{"cell_type":"markdown","metadata":{},"source":["### Model 2\n","Normals are concatenated to the output of the DGCNN of a single pointcloud and passed through a linear layer before being concatenated to the other pointcloud and normal; then 2 final classification layers act as classifier. This is the model that returns the best performances overall."]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T15:32:00.857757Z","iopub.status.busy":"2022-05-12T15:32:00.857469Z","iopub.status.idle":"2022-05-12T15:32:00.884968Z","shell.execute_reply":"2022-05-12T15:32:00.884125Z","shell.execute_reply.started":"2022-05-12T15:32:00.857729Z"},"trusted":true},"outputs":[],"source":["\n","class DGCNN_cls(nn.Module):\n","    def __init__(self, args, output_channels=1):\n","        super(DGCNN_cls, self).__init__()\n","        self.args = args\n","        self.device=\"cuda\" if self.args[\"cuda\"] else \"cpu\"\n","        self.k = args[\"k\"]\n","        \n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.bn3 = nn.BatchNorm2d(128)\n","        self.bn4 = nn.BatchNorm2d(256)\n","        self.bn5 = nn.BatchNorm1d(args[\"emb_dims\"])\n","\n","        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n","                                   self.bn1,\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        self.conv2 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n","                                   self.bn2,\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 128, kernel_size=1, bias=False),\n","                                   self.bn3,\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        self.conv4 = nn.Sequential(nn.Conv2d(128*2, 256, kernel_size=1, bias=False),\n","                                   self.bn4,\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        self.conv5 = nn.Sequential(nn.Conv1d(512, args[\"emb_dims\"], kernel_size=1, bias=False),\n","                                   self.bn5,\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        \n","        self.node_num=args[\"final_dim_DGCNN\"]\n","        dim_normal=args[\"num_points\"]*3\n","        \n","        self.linear1 = nn.Linear(args[\"emb_dims\"]*2+dim_normal, self.node_num, bias=False)\n","        self.bn6 = nn.BatchNorm1d(self.node_num)\n","\n","        self.dp1 = nn.Dropout(p=args[\"dropout\"])\n","        self.dp3=nn.Dropout(p=args[\"dropout\"])\n","        self.pointDropout=PointDropout(args[\"point_dropout\"])\n","        \n","        #classification layers\n","        self.linearAlternative = nn.Linear(self.node_num*2, args[\"dim_classification\"], bias=False)\n","        self.bnAlt = nn.BatchNorm1d(args[\"dim_classification\"])\n","        self.linearAlternative3 = nn.Linear(args[\"dim_classification\"], output_channels, bias=False)\n","       \n","\n","    \n","    def dgcnn_step(self,x,n):\n","        batch_size = x.size(0)\n","        x = get_graph_feature(x, k=self.k,device=self.device)           # (batch_size, 3, num_points) -> (batch_size, 3*2, num_points, k)\n","        x = self.conv1(x)                                               # (batch_size, 3*2, num_points, k) -> (batch_size, 64, num_points, k)\n","        x1 = x.max(dim=-1, keepdim=False)[0]                            # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n","\n","        x = get_graph_feature(x1, k=self.k,device=self.device)          # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n","        x = self.conv2(x)                                               # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n","        x2 = x.max(dim=-1, keepdim=False)[0]                            # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n","\n","        x = get_graph_feature(x2, k=self.k,device=self.device)          # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n","        x = self.conv3(x)                                               # (batch_size, 64*2, num_points, k) -> (batch_size, 128, num_points, k)\n","        x3 = x.max(dim=-1, keepdim=False)[0]                            # (batch_size, 128, num_points, k) -> (batch_size, 128, num_points)\n","\n","        x = get_graph_feature(x3, k=self.k,device=self.device)          # (batch_size, 128, num_points) -> (batch_size, 128*2, num_points, k)\n","        x = self.conv4(x)                                               # (batch_size, 128*2, num_points, k) -> (batch_size, 256, num_points, k)\n","        x4 = x.max(dim=-1, keepdim=False)[0]                            # (batch_size, 256, num_points, k) -> (batch_size, 256, num_points)\n","\n","        x = torch.cat((x1, x2, x3, x4), dim=1)                          # (batch_size, 64+64+128+256, num_points)\n","\n","        x = self.conv5(x)                                               # (batch_size, 64+64+128+256, num_points) -> (batch_size, emb_dims, num_points)\n","        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)           # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims)\n","        x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)           # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims)\n","        x = torch.cat((x1, x2), 1)                                      # (batch_size, emb_dims*2)\n","        \n","        n=torch.flatten(n,start_dim=1)\n","        x=torch.cat((x,n),dim=1)\n","        x = F.leaky_relu(self.bn6(self.linear1(x))) # output is (batch_size, final_dim_DGCNN)\n","        x = self.dp1(x)\n","        return x\n","\n","    def forward(self, x,n,invert=False):\n","       \n","        if invert:\n","            #use inverted pair\n","            first=x[:,1,:,:]\n","            second=x[:,0,:,:]\n","            second_n=n[:,0,:,:]\n","            first_n=n[:,1,:,:]\n","        else:\n","            first=x[:,0,:,:]\n","            first_n=n[:,0,:,:]\n","            second=x[:,1,:,:]\n","            second_n=n[:,1,:,:]\n","        \n","\n","        first,first_n=self.pointDropout(first,first_n)\n","        second,second_n=self.pointDropout(second,second_n)\n","\n","        #compute DGCNN output from both pointclouds \n","        ret1=self.dgcnn_step(first,first_n)      \n","        ret2=self.dgcnn_step(second,second_n)\n","\n","        #combine output and use classifier\n","        x=torch.cat((ret1,ret2),dim=1)\n","        x=F.leaky_relu(self.bnAlt(self.linearAlternative(x)))\n","        x=self.dp3(x)\n","        x=self.linearAlternative3(x)\n","        \n","\n","        return x\n"]},{"cell_type":"markdown","metadata":{},"source":["### Alternative models without normals"]},{"cell_type":"markdown","metadata":{},"source":["#### Model 3 (without normals)\n"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T15:32:00.896573Z","iopub.status.busy":"2022-05-12T15:32:00.896067Z","iopub.status.idle":"2022-05-12T15:32:00.924608Z","shell.execute_reply":"2022-05-12T15:32:00.923847Z","shell.execute_reply.started":"2022-05-12T15:32:00.896532Z"},"trusted":true},"outputs":[],"source":["class DGCNN_cls_no_normals(nn.Module):\n","    def __init__(self, args, output_channels=1):\n","        super(DGCNN_cls_no_normals, self).__init__()\n","        self.args = args\n","        self.device=\"cuda\" if self.args[\"cuda\"] else \"cpu\"\n","        self.k = args[\"k\"]\n","        \n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.bn3 = nn.BatchNorm2d(128)\n","        self.bn4 = nn.BatchNorm2d(256)\n","        self.bn5 = nn.BatchNorm1d(args[\"emb_dims\"])\n","\n","        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n","                                   self.bn1,\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        self.conv2 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n","                                   self.bn2,\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 128, kernel_size=1, bias=False),\n","                                   self.bn3,\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        self.conv4 = nn.Sequential(nn.Conv2d(128*2, 256, kernel_size=1, bias=False),\n","                                   self.bn4,\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        self.conv5 = nn.Sequential(nn.Conv1d(512, args[\"emb_dims\"], kernel_size=1, bias=False),\n","                                   self.bn5,\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        self.linear1 = nn.Linear(args[\"emb_dims\"]*2, 512, bias=False)\n","        self.bn6 = nn.BatchNorm1d(512)\n","        self.dp1 = nn.Dropout(p=args[\"dropout\"])\n","        self.linear2 = nn.Linear(512, 256)\n","        self.bn7 = nn.BatchNorm1d(256)\n","        self.dp2 = nn.Dropout(p=args[\"dropout\"])\n","        \n","        self.linear3 = nn.Linear(256, args[\"final_dim_DGCNN\"])\n","        self.bn8 = nn.BatchNorm1d( args[\"final_dim_DGCNN\"])\n","        self.dp3=nn.Dropout(p=args[\"dropout\"])\n","       \n","\n","        #final classification layer\n","        if args[\"dim_classification\"]>0:\n","            self.beginClassification = nn.Linear(args[\"final_dim_DGCNN\"]*2, args[\"dim_classification\"])\n","            self.bn9 = nn.BatchNorm1d(  args[\"dim_classification\"] )\n","            self.midClassification = nn.Linear(args[\"dim_classification\"], args[\"dim_classification\"])\n","            self.bn10 = nn.BatchNorm1d(  args[\"dim_classification\"] )\n","            self.endClassification = nn.Linear(args[\"dim_classification\"] , output_channels)\n","\n","        else:\n","            self.classification= nn.Linear(args[\"final_dim_DGCNN\"]*2, output_channels)\n","\n","\n","    \n","    def dgcnn_step(self,x):\n","        batch_size = x.size(0)\n","        x = get_graph_feature(x, k=self.k,device=self.device)           # (batch_size, 3, num_points) -> (batch_size, 3*2, num_points, k)\n","        x = self.conv1(x)                                               # (batch_size, 3*2, num_points, k) -> (batch_size, 64, num_points, k)\n","        x1 = x.max(dim=-1, keepdim=False)[0]                            # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n","\n","        x = get_graph_feature(x1, k=self.k,device=self.device)          # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n","        x = self.conv2(x)                                               # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n","        x2 = x.max(dim=-1, keepdim=False)[0]                            # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n","\n","        x = get_graph_feature(x2, k=self.k,device=self.device)          # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n","        x = self.conv3(x)                                               # (batch_size, 64*2, num_points, k) -> (batch_size, 128, num_points, k)\n","        x3 = x.max(dim=-1, keepdim=False)[0]                            # (batch_size, 128, num_points, k) -> (batch_size, 128, num_points)\n","\n","        x = get_graph_feature(x3, k=self.k,device=self.device)          # (batch_size, 128, num_points) -> (batch_size, 128*2, num_points, k)\n","        x = self.conv4(x)                                               # (batch_size, 128*2, num_points, k) -> (batch_size, 256, num_points, k)\n","        x4 = x.max(dim=-1, keepdim=False)[0]                            # (batch_size, 256, num_points, k) -> (batch_size, 256, num_points)\n","\n","        x = torch.cat((x1, x2, x3, x4), dim=1)                          # (batch_size, 64+64+128+256, num_points)\n","\n","        x = self.conv5(x)                                               # (batch_size, 64+64+128+256, num_points) -> (batch_size, emb_dims, num_points)\n","        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)           # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims)\n","        x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)           # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims)\n","        x = torch.cat((x1, x2), 1)                                      # (batch_size, emb_dims*2)\n","\n","        x = F.leaky_relu(self.bn6(self.linear1(x)), negative_slope=0.2) # (batch_size, emb_dims*2) -> (batch_size, 512)\n","        x = self.dp1(x)\n","        x = F.leaky_relu(self.bn7(self.linear2(x)), negative_slope=0.2) # (batch_size, 512) -> (batch_size, 256)\n","        x = self.dp2(x)\n","        x =  F.leaky_relu(self.bn8(self.linear3(x)), negative_slope=0.2) # (batch_size, 256) -> (batch_size, output_channels)\n","        x=self.dp3(x)\n","        return x\n","\n","    def forward(self, x,n,invert=False):\n","        if invert:\n","            first=x[:,1,:,:]\n","            second=x[:,0,:,:]\n","        else:\n","            first=x[:,0,:,:]\n","            second=x[:,1,:,:]\n","\n","        #compute DGCNN output from both pointclouds \n","        ret1=self.dgcnn_step(first)      \n","        ret2=self.dgcnn_step(second)\n","\n","        #combine output and use classifier\n","        x=torch.cat((ret1,ret2),dim=1)\n","        if self.args[\"dim_classification\"]>0:\n","            x=F.leaky_relu(self.bn9(self.beginClassification(x)), negative_slope=0.2)\n","            x=self.dp3(x)\n","            x=F.leaky_relu(self.bn10(self.midClassification(x)), negative_slope=0.2)\n","            x=self.dp3(x)\n","            x=self.endClassification(x)\n","        else: \n","            x=self.classification(x)\n","\n","        return x\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Model 4 (without normals)\n","Thsi model doesn't use the normals and the dgcnn net acts only as a feature extractor, it doesn't use linear layers.\n","The linear layers are only used after with both pointcloud features and act as classifier."]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T15:32:00.933485Z","iopub.status.busy":"2022-05-12T15:32:00.932945Z","iopub.status.idle":"2022-05-12T15:32:00.958527Z","shell.execute_reply":"2022-05-12T15:32:00.957680Z","shell.execute_reply.started":"2022-05-12T15:32:00.933452Z"},"trusted":true},"outputs":[],"source":["class DGCNN_cls2_no_normals(nn.Module):\n","    def __init__(self, args, output_channels=1):\n","        super(DGCNN_cls2_no_normals, self).__init__()\n","        self.args = args\n","        self.device=\"cuda\" if self.args[\"cuda\"] else \"cpu\"\n","        self.k = args[\"k\"]\n","        \n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.bn3 = nn.BatchNorm2d(128)\n","        self.bn4 = nn.BatchNorm2d(256)\n","        self.bn5 = nn.BatchNorm1d(args[\"emb_dims\"])\n","\n","        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n","                                   self.bn1,\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        self.conv2 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n","                                   self.bn2,\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 128, kernel_size=1, bias=False),\n","                                   self.bn3,\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        self.conv4 = nn.Sequential(nn.Conv2d(128*2, 256, kernel_size=1, bias=False),\n","                                   self.bn4,\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        self.conv5 = nn.Sequential(nn.Conv1d(512, args[\"emb_dims\"], kernel_size=1, bias=False),\n","                                   self.bn5,\n","                                   nn.LeakyReLU(negative_slope=0.2))\n","        self.dp3=nn.Dropout(p=args[\"dropout\"])\n","\n","        #final classification layer\n","        if args[\"dim_classification\"]>0:\n","            self.beginClassification = nn.Linear(args[\"emb_dims\"]*4, args[\"dim_classification\"])\n","            self.bn9 = nn.BatchNorm1d(  args[\"dim_classification\"] )\n","            self.midClassification = nn.Linear(args[\"dim_classification\"], args[\"dim_classification\"])\n","            self.bn10 = nn.BatchNorm1d(  args[\"dim_classification\"] )\n","            self.endClassification = nn.Linear(args[\"dim_classification\"] , output_channels)\n","\n","        else:\n","            self.classification= nn.Linear(args[\"final_dim_DGCNN\"]*2, output_channels)\n","\n","\n","    \n","    def dgcnn_step(self,x):\n","        batch_size = x.size(0)\n","        x = get_graph_feature(x, k=self.k,device=self.device)           # (batch_size, 3, num_points) -> (batch_size, 3*2, num_points, k)\n","        x = self.conv1(x)                                               # (batch_size, 3*2, num_points, k) -> (batch_size, 64, num_points, k)\n","        x1 = x.max(dim=-1, keepdim=False)[0]                            # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n","\n","        x = get_graph_feature(x1, k=self.k,device=self.device)          # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n","        x = self.conv2(x)                                               # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n","        x2 = x.max(dim=-1, keepdim=False)[0]                            # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n","\n","        x = get_graph_feature(x2, k=self.k,device=self.device)          # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n","        x = self.conv3(x)                                               # (batch_size, 64*2, num_points, k) -> (batch_size, 128, num_points, k)\n","        x3 = x.max(dim=-1, keepdim=False)[0]                            # (batch_size, 128, num_points, k) -> (batch_size, 128, num_points)\n","\n","        x = get_graph_feature(x3, k=self.k,device=self.device)          # (batch_size, 128, num_points) -> (batch_size, 128*2, num_points, k)\n","        x = self.conv4(x)                                               # (batch_size, 128*2, num_points, k) -> (batch_size, 256, num_points, k)\n","        x4 = x.max(dim=-1, keepdim=False)[0]                            # (batch_size, 256, num_points, k) -> (batch_size, 256, num_points)\n","\n","        x = torch.cat((x1, x2, x3, x4), dim=1)                          # (batch_size, 64+64+128+256, num_points)\n","\n","        x = self.conv5(x)                                               # (batch_size, 64+64+128+256, num_points) -> (batch_size, emb_dims, num_points)\n","        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)           # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims)\n","        x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)           # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims)\n","        x = torch.cat((x1, x2), 1)                                      # (batch_size, emb_dims*2)\n","\n","        return x\n","\n","    def forward(self, x,invert=False):\n","        if invert:\n","            first=x[:,1,:,:]\n","            second=x[:,0,:,:]\n","        else:\n","            first=x[:,0,:,:]\n","            second=x[:,1,:,:]\n","\n","        #compute DGCNN output from both pointclouds \n","        ret1=self.dgcnn_step(first)      \n","        ret2=self.dgcnn_step(second)\n","\n","        #combine output and use classifier\n","        x=torch.cat((ret1,ret2),dim=1)\n","        if self.args[\"dim_classification\"]>0:\n","            x=F.leaky_relu(self.bn9(self.beginClassification(x)), negative_slope=0.2)\n","            x=self.dp3(x)\n","            x=F.leaky_relu(self.bn10(self.midClassification(x)), negative_slope=0.2)\n","            x=self.dp3(x)\n","            x=self.endClassification(x)\n","        else: \n","            x=self.classification(x)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T15:32:00.967978Z","iopub.status.busy":"2022-05-12T15:32:00.967463Z","iopub.status.idle":"2022-05-12T15:32:00.974468Z","shell.execute_reply":"2022-05-12T15:32:00.973493Z","shell.execute_reply.started":"2022-05-12T15:32:00.967949Z"},"trusted":true},"outputs":[],"source":["#compute predictions and update loss and optimizer\n","def train_step(model,opt,criterion,data,label,normals,invert=False):\n","    \n","    data = data.permute(0,1,3,2)\n","    normals=normals.permute(0,1,3,2)\n","    opt.zero_grad()\n","    logits = model(data,normals,invert)\n","    logits=torch.squeeze(torch.sigmoid(logits))\n","    loss = criterion(logits, label)\n","    loss.backward()\n","    opt.step()\n","    preds=(logits>0.5).int()\n","    \n","    return preds,loss"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T15:32:01.006581Z","iopub.status.busy":"2022-05-12T15:32:01.006395Z","iopub.status.idle":"2022-05-12T15:32:01.012180Z","shell.execute_reply":"2022-05-12T15:32:01.011329Z","shell.execute_reply.started":"2022-05-12T15:32:01.006558Z"},"trusted":true},"outputs":[],"source":["#get some metrics and put in dictionary, useful for debug and logging with wandb\n","def get_metrics(train_true,train_pred,loss,epoch,suff=\"train\"):\n","    \n","    acc=metrics.accuracy_score(train_true, train_pred)\n","    avg_acc= metrics.balanced_accuracy_score(train_true, train_pred)\n","    precision=metrics.precision_score(train_true,train_pred)\n","    recall=metrics.recall_score(train_true, train_pred)\n","    f1=metrics.f1_score(train_true,train_pred)\n","\n","    d={f\"{suff}_acc\":acc,f\"{suff}_avg_acc\":avg_acc,f\"{suff}_precision\":precision,f\"{suff}_recall\":recall,\n","        f\"{suff}_f1\":f1,f\"{suff}_loss\":loss,\"epoch\":epoch}\n","    return d\n"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T15:32:01.043870Z","iopub.status.busy":"2022-05-12T15:32:01.043550Z","iopub.status.idle":"2022-05-12T15:32:01.049701Z","shell.execute_reply":"2022-05-12T15:32:01.049045Z","shell.execute_reply.started":"2022-05-12T15:32:01.043839Z"},"trusted":true},"outputs":[],"source":["#log weights of the layers, could lead to some interesting insights\n","def log_weights(model,epoch):\n","    weights=model.state_dict()\n","    log={}\n","    count=0\n","    for key in weights.keys():\n","        if \"bn\" not in key and (\"weight\" in key or \"bias\" in key):\n","            if \"linear\" in key.lower():\n","                log[key]=weights[key]\n","                \n","                mean=torch.mean(weights[key])\n","                var=torch.var(weights[key])\n","                std=torch.std(weights[key])\n","                log[f\"{key}_mean\"]=mean\n","                log[f\"{key}_var\"]=var\n","                log[f\"{key}_std\"]=std\n","    log[\"epoch\"]=epoch\n","    return log"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T15:32:01.105511Z","iopub.status.busy":"2022-05-12T15:32:01.105019Z","iopub.status.idle":"2022-05-12T15:32:01.140821Z","shell.execute_reply":"2022-05-12T15:32:01.140106Z","shell.execute_reply.started":"2022-05-12T15:32:01.105481Z"},"trusted":true},"outputs":[],"source":["\n","def train(args,extra_args, io):\n","    # num_workers=0 to make it work on windows\n","    train_loader = DataLoader(MyDataset(args,extra_args,partition='train'), \n","            num_workers=extra_args[\"num_workers\"],batch_size=args[\"batch_size\"], shuffle=True, drop_last=True)\n","    test_loader = DataLoader(MyDataset(args,extra_args,partition='test'), \n","            num_workers=extra_args[\"num_workers\"],batch_size=args[\"test_batch_size\"], shuffle=True, drop_last=False)\n","\n","    device = torch.device(\"cuda\" if args[\"cuda\"] else \"cpu\")\n","    model = DGCNN_cls(args).to(device)\n","\n","    print(\"Model loaded!\")\n","\n","    #not used, but useful if using multiple GPUs\n","    model = nn.DataParallel(model)\n","    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n","    \n","    #choose optimizer\n","    if args[\"optimizer\"].lower()==\"sgd\":\n","        print(\"Use SGD\")\n","        opt = optim.SGD(model.parameters(), lr=args[\"lr\"]*100,\n","                momentum=args[\"momentum\"], weight_decay=args[\"weight_decay\"])\n","    elif args[\"optimizer\"].lower()==\"adam\":\n","        print(\"Use Adam\")\n","        opt = optim.Adam(model.parameters(), lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n","\n","    #choose scheduler\n","    if args[\"scheduler\"] == 'cos':\n","        scheduler = CosineAnnealingLR(opt, args[\"epochs\"], eta_min=0,verbose=True)\n","    elif args[\"scheduler\"] == 'step':\n","        scheduler = StepLR(opt, step_size=30, gamma=0.5,verbose=True)\n","    elif args[\"scheduler\"].lower() ==\"reducelrplateau\":\n","        scheduler = ReduceLROnPlateau(opt, 'min',factor=0.5,patience=3)\n","    \n","    criterion = cal_loss\n","\n","    if extra_args[\"wand_project_name\"]:\n","        if extra_args[\"wandb_resume\"]:\n","            wandb.init(\n","                \n","                project=extra_args[\"wand_project_name\"], \n","                id=extra_args[\"wandb_resume\"],\n","                resume=\"allow\",\n","                entity=\"parmola\",\n","                config=args)\n","        else:\n","            wandb.init(\n","                project=extra_args[\"wand_project_name\"], \n","                entity=\"parmola\",\n","                config=args)\n","\n","    start_epoch=0\n","    #load saved model\n","    if extra_args[\"load_path\"]:\n","        checkpoint = torch.load(extra_args[\"load_path\"])\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        scheduler.load_state_dict(checkpoint['scheduler'])\n","        opt.load_state_dict(checkpoint['optimizer_state_dict'])\n","        start_epoch = checkpoint['epoch']\n","\n","    print(args)\n","    \n","    # TRAINING\n","    best_train_acc=0\n","    best_test_acc=0\n","    for epoch in range(start_epoch,args[\"epochs\"]):\n","        train_loss = 0.0\n","        count = 0.0\n","        model.train()\n","        train_pred = []\n","        train_true = []\n","        print(\"Starting epoch \",epoch)\n","        for data,normals, label in train_loader:\n","            #print(\"get data\")\n","            data, label = data.to(device), label.to(device).squeeze()\n","            normals=normals.to(device).float()\n","            data=data.float()\n","            label=label.float()\n","            batch_size = data.size()[0]\n","            \n","            count += batch_size\n","            preds,loss=train_step(model,opt,criterion,data,label,normals,invert=False)\n","            train_loss += loss.item() * batch_size\n","            train_true.append(label.cpu().numpy())\n","            train_pred.append(preds.detach().cpu().numpy())\n","            \n","            #train also using inverted pairs of the point clouds\n","            if extra_args[\"inverted_pair\"]:\n","                count += batch_size\n","                preds,loss=train_step(model,opt,criterion,data,label,invert=True)\n","                train_loss += loss.item() * batch_size\n","                train_true.append(label.cpu().numpy())\n","                train_pred.append(preds.detach().cpu().numpy())\n","            \n","        \n","        if args[\"scheduler\"] == 'cos':\n","            scheduler.step()\n","        elif args[\"scheduler\"] == 'step':\n","            if opt.param_groups[0]['lr'] > 1e-5:\n","                scheduler.step()\n","            if opt.param_groups[0]['lr'] < 1e-5:\n","                for param_group in opt.param_groups:\n","                    param_group['lr'] = 1e-5        \n","        elif args[\"scheduler\"].lower() ==\"reducelrplateau\":\n","            scheduler.step(train_loss*1.0/count)\n","        try:\n","            actual_lr=scheduler.get_last_lr()\n","        except:\n","            actual_lr=opt.param_groups[0]['lr']\n","        if extra_args[\"wand_project_name\"]:\n","            wandb.log({\"actual_lr\": actual_lr,\"epoch\":epoch})\n","\n","        train_true = np.concatenate(train_true)\n","        train_pred = np.concatenate(train_pred)\n","        \n","        scores=get_metrics(train_true,train_pred,train_loss*1.0/count,epoch,\"train\")\n","        \n","        outstr = 'Train %d, loss: %.6f, train acc: %.6f, f1: %.6f' % (epoch,train_loss*1.0/count,\n","                                                                        scores[\"train_acc\"],\n","                                                                        scores[\"train_f1\"])\n","        io.cprint(outstr)\n","        \n","\n","        if extra_args[\"wand_project_name\"]:\n","            wandb.log(scores)\n","            if  scores[\"train_acc\"]>best_train_acc:\n","                best_train_acc= scores[\"train_acc\"]\n","                wandb.log({\"best_train_acc\": best_train_acc,\"epoch\":epoch})\n","                \n","            weights_log=log_weights(model,epoch)\n","            #wandb.log(weights_log)\n","\n","        ####################\n","        # Test\n","        ####################\n","        test_loss = 0.0\n","        count = 0.0\n","        model.eval()\n","        test_pred = []\n","        test_true = []\n","        for data,normals, label in test_loader:\n","            data, label = data.to(device), label.to(device).squeeze()\n","            data=data.float()\n","            label=label.float()\n","            normals=normals.to(device).float()\n","            batch_size = data.size()[0]\n","            data = data.permute(0,1,3,2)\n","            logits = model(data,normals)\n","            logits=torch.squeeze(torch.sigmoid(logits))\n","            loss = criterion(logits, label)\n","            preds=(logits>0.5).int()\n","    \n","\n","            count += batch_size\n","            test_loss += loss.item() * batch_size\n","            test_true.append(label.cpu().numpy())\n","            test_pred.append(preds.detach().cpu().numpy())\n","        \n","        test_true = np.concatenate(test_true)\n","        test_pred = np.concatenate(test_pred)\n","\n","        scores=get_metrics(test_true,test_pred,test_loss*1.0/count,epoch,\"test\")\n","        \n","        outstr = 'Test %d, loss: %.6f, test acc: %.6f, test f1: %.6f' %(epoch,test_loss*1.0/count,\n","                                                                        scores[\"test_acc\"],\n","                                                                        scores[\"test_f1\"])\n","        \n","        io.cprint(outstr)\n","        if extra_args[\"wand_project_name\"]:\n","            wandb.log(scores)\n","            if scores[\"test_acc\"]>best_test_acc:\n","                best_test_acc=scores[\"test_acc\"]\n","                wandb.log({\"best_test_acc\": best_test_acc,\"epoch\":epoch})\n","        \n","        if epoch % extra_args[\"saving_step\"]==0:\n","            file_path=f\"model_{epoch}of{args['epochs']}epoch_{args['info']}.pt\"\n","            file_path=os.path.join(\"/kaggle/working\",file_path)\n","            \n","            f=open(file_path,'w')\n","            \n","            #save model and optimizer\n","            torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': opt.state_dict(),\n","            'scheduler': scheduler.state_dict(),\n","            }, file_path)\n","            if extra_args[\"wand_project_name\"]:\n","                wandb.save(file_path)\n","             \n","        #end epoch\n","        \n","    \n","    if extra_args[\"wand_project_name\"]: wandb.finish()\n","\n"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T15:32:01.159568Z","iopub.status.busy":"2022-05-12T15:32:01.159384Z","iopub.status.idle":"2022-05-12T15:32:01.170297Z","shell.execute_reply":"2022-05-12T15:32:01.169604Z","shell.execute_reply.started":"2022-05-12T15:32:01.159545Z"},"trusted":true},"outputs":[],"source":["\n","def test(args,extra_args, io):\n","        test_loader = DataLoader(MyDataset(args,extra_args,partition='val'), \n","                num_workers=extra_args[\"num_workers\"],batch_size=args[\"batch_size\"], shuffle=True, drop_last=True)\n","\n","        device = torch.device(\"cuda\" if args[\"cuda\"] else \"cpu\")\n","\n","        model = DGCNN_cls(args).to(device)\n","\n","        model = nn.DataParallel(model)\n","       \n","        model.load_state_dict(torch.load(args[\"model_path\"]))\n","        model = model.eval()\n","        test_acc = 0.0\n","        count = 0.0\n","        test_true = []\n","        test_pred = []\n","        for data,normals, label in test_loader:\n","                data, label = data.to(device), label.to(device).squeeze()\n","                data=data.float()\n","                label=label.float()\n","                normals=normals.to(device).float()\n","                batch_size = data.size()[0]\n","                data = data.permute(0,1,3,2)\n","                logits = model(data,normals)\n","                logits=torch.squeeze(torch.sigmoid(logits))\n","                preds=(logits>0.5).int()\n","                \n","                count += batch_size\n","                test_true.append(label.cpu().numpy())\n","                test_pred.append(preds.detach().cpu().numpy())\n","\n","                #useless to log these\n","                test_loss=0\n","                epoch=args[\"epochs\"]\n","                \n","        scores=get_metrics(test_true,test_pred,test_loss,epoch,\"test\")\n","\n","        outstr = 'Test %d, loss: %.6f, test acc: %.6f, test f1: %.6f' %(epoch,test_loss*1.0/count,\n","                                                                        scores[\"test_acc\"],\n","                                                                        scores[\"test_f1\"])\n","        io.cprint(outstr)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Main"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","def _init_():\n","    if not os.path.exists('outputs'):\n","        os.makedirs('outputs')\n","    if not os.path.exists('outputs/'+extra_args[\"exp_name\"]):\n","        os.makedirs('outputs/'+extra_args[\"exp_name\"])\n","    if not os.path.exists('outputs/'+extra_args[\"exp_name\"]+'/'+'models'):\n","        os.makedirs('outputs/'+extra_args[\"exp_name\"]+'/'+'models')\n","\n","if __name__ == \"__main__\":\n","\n","    #stats I don't need to log\n","    extra_args={\n","        \"exp_name\": \"exp\",              \n","        \"wand_project_name\": \"\",\n","        \"load_path\":\"\",                     #path to load\n","        \"wandb_resume\":\"\",                  #id of the run to resume\n","        \"dataset_file_path\": \"datasets\\dataset_179608pairs_250points_center_random_normals_chunk_alpha1.hdf5\",\n","        \"eval\": False,\n","        \"saving_step\":10,                   #how much epochs between savings\n","        \"num_workers\": 2,                   # parallel workers on dataloader, 0 if windows\n","        \"inverted_pair\": False,             #if true, the dataset use also the inverted pair of fragment for the training\n","    }\n","\n","    #Stats I need to log\n","    args={\n","        #general\n","        \"info\": \"new_dataset_best_parameters_new_lr\",\n","\n","        \"num_train_samples\": 2000,     # Num of pairs to consider for train\n","        \"num_test_samples\":1000,       # Num of pairs to consider for test\n","        \"batch_size\": 32,               # Size of batch\n","        \"test_batch_size\": 32,          # Size of batch\n","        \"num_workers\": 2,               # parallel workers on dataloader, 0 if windows\n","\n","        #model\n","        \"architecture\": \"DGCNN\",    \n","        \"num_points\": 250,          # num of points to use\n","        \"dropout\": 0.3,             # initial dropout rate\n","        \"point_dropout\":0.2,\n","        \"emb_dims\": 1024,           # Dimension of embeddings\n","        \"k\": 30,                    # Num of nearest neighbors to use\n","        \"final_dim_DGCNN\": 512,     #dimension of final linear layer of DGCNN, max=256\n","        \"dim_classification\": 256,   #dim of mid final classification layer, max= final_dim_DGCNN*2\n","\n","        #training\n","        \"epochs\": 50,               # number of episode to train\n","        \"optimizer\": \"adam\",        # optimizer [adam,sgd]\n","        \"lr\": 1e-4,                # learning rate (default: 0.001, 0.1 if using sgd)\n","        \"momentum\": 0.9,            # SGD momentum (default: 0.9)\n","        \"scheduler\": \"reducelrplateau\",         # Scheduler to use, [cos, step,reducelrplateau]\n","        \"weight_decay\": 1e-4,       # weight_decay for optimizer\n","        \"seed\": 1,                  # random seed (default: 1)\n","    }\n","    \n","    _init_()\n","\n","    io = IOStream('outputs/' + extra_args[\"exp_name\"] + '/run.log')\n","    io.cprint(str(args))\n","    \n","    #set up GPU\n","    args[\"cuda\"] = torch.cuda.is_available()\n","    torch.manual_seed(args[\"seed\"])\n","    if args[\"cuda\"]:\n","        io.cprint(\n","            'Using GPU : ' + str(torch.cuda.current_device()) + ' from ' + str(torch.cuda.device_count()) + ' devices')\n","        torch.cuda.manual_seed(args[\"seed\"])\n","    else:\n","        io.cprint('Using CPU')\n","\n","    if not extra_args[\"eval\"]:\n","        train(args,extra_args, io)\n","        wandb.finish()\n","        torch.cuda.empty_cache()\n","            \n","    else:\n","        test(args,extra_args, io)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
