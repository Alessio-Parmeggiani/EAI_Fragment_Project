{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import open3d as o3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "#import argparse\n",
    "#import copy\n",
    "#import math\n",
    "#import sys\n",
    "#import json\n",
    "import glob\n",
    "import h5py\n",
    "#import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.init as init\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from description file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pretty_print(grid):\n",
    "    for row in grid:\n",
    "        print(row)\n",
    "\n",
    "#from path of description file get the coupling grid, model data and \n",
    "#remove models with error\n",
    "def process_file(path):\n",
    "    with open(path,'r') as f:\n",
    "        #don't consider first two lines\n",
    "        f.readline()           #segmenti x x x\n",
    "        f.readline()           #rotazioni x x x\n",
    "\n",
    "        line=f.readline()      #numero pezzi x  \n",
    "\n",
    "        #get number of fragments\n",
    "        number=int(re.findall(\"\\d+\",line)[0])\n",
    "\n",
    "        #coupling matrix\n",
    "        grid=[]\n",
    "        for i in range(number):\n",
    "            line=f.readline()\n",
    "            ret=re.findall('-1|0|1',line)\n",
    "            grid.append(list(map(int,ret)))\n",
    "\n",
    "        #model data\n",
    "        model_names=[]\n",
    "        non_valid_indeces=[]\n",
    "        for m in range(number):\n",
    "            f.readline()        #blank line\n",
    "            name=f.readline()   #model name\n",
    "            mesh=f.readline()   #mesh n\n",
    "            f.readline()        #external n\n",
    "            f.readline()        #internal n\n",
    "\n",
    "            #if mesh=0 I don't consider the element\n",
    "            mesh_n=int(mesh.rstrip().split(\" \")[1]) \n",
    "            if mesh_n!=0: \n",
    "                #try create file name\n",
    "                file_name= name.rstrip().replace(\".\",\"_\")\n",
    "                model_names.append(file_name)\n",
    "                \n",
    "            else:\n",
    "                #saving indices to remove later\n",
    "                non_valid_indeces.append(m)\n",
    "\n",
    "        #removing elements from grid\n",
    "        #sorted in reverse to avoid wrong index\n",
    "        for index in sorted(non_valid_indeces, reverse=True):\n",
    "            del grid[index]\n",
    "            for row in grid:\n",
    "                del row[index]\n",
    "        \n",
    "        return grid,model_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set(folder,verbose=True):\n",
    "    folder_path=os.path.join(main_path,folder)\n",
    "    models_file=[]\n",
    "\n",
    "    #for each set of pieces\n",
    "    for file in os.listdir(folder_path):\n",
    "        if '.txt' in file:\n",
    "            description_file=file\n",
    "        elif '.stl' in file:\n",
    "            #not used, they are not in the same order of the file\n",
    "            models_file.append(file)\n",
    "    #parsing description file\n",
    "    full_description_file=os.path.join(folder_path,description_file)\n",
    "    grid,model_names=process_file(full_description_file)\n",
    "\n",
    "    #get path of models of current set\n",
    "    model_prefix=folder.replace(\"generatedTest_\",\"\")\n",
    "    complete_models_path=[]\n",
    "    for i in range(len(model_names)):\n",
    "        #e.g. 2021_11_29_10_00_40_Cube_001.stl\n",
    "        correct_model_name=f\"{model_prefix}_{model_names[i]}.stl\"\n",
    "\n",
    "        #saving only names, could be useful\n",
    "        model_names[i]=correct_model_name\n",
    "        complete_models_path.append(os.path.join(folder_path,correct_model_name))\n",
    "\n",
    "    #create set and put into list of sets\n",
    "    set={\"models\":complete_models_path,\"grid\":grid}\n",
    "    if verbose:\n",
    "        print(\"A set: \\n\")\n",
    "        print(\"Models: \",set[\"models\"])\n",
    "        print(\"Grid: \")\n",
    "        pretty_print(set[\"grid\"])\n",
    "\n",
    "    return set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here i save all the sets of fragments\n",
    "sets=[]\n",
    "\n",
    "#root folder\n",
    "main_path=\"produzione_29112021\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A set: \n",
      "\n",
      "Models:  ['produzione_29112021\\\\generatedTest_2021_11_29_10_00_40\\\\2021_11_29_10_00_40_Cube.stl', 'produzione_29112021\\\\generatedTest_2021_11_29_10_00_40\\\\2021_11_29_10_00_40_Cube_001.stl', 'produzione_29112021\\\\generatedTest_2021_11_29_10_00_40\\\\2021_11_29_10_00_40_Cube_002.stl', 'produzione_29112021\\\\generatedTest_2021_11_29_10_00_40\\\\2021_11_29_10_00_40_Cube_003.stl', 'produzione_29112021\\\\generatedTest_2021_11_29_10_00_40\\\\2021_11_29_10_00_40_Cube_004.stl', 'produzione_29112021\\\\generatedTest_2021_11_29_10_00_40\\\\2021_11_29_10_00_40_Cube_005.stl', 'produzione_29112021\\\\generatedTest_2021_11_29_10_00_40\\\\2021_11_29_10_00_40_Cube_006.stl', 'produzione_29112021\\\\generatedTest_2021_11_29_10_00_40\\\\2021_11_29_10_00_40_Cube_007.stl']\n",
      "Grid: \n",
      "[-1, 1, 0, 0, 1, 1, 1, 1]\n",
      "[1, -1, 1, 0, 1, 1, 1, 0]\n",
      "[0, 1, -1, 1, 1, 1, 1, 0]\n",
      "[0, 0, 1, -1, 1, 0, 1, 0]\n",
      "[1, 1, 1, 1, -1, 0, 1, 1]\n",
      "[1, 1, 1, 0, 0, -1, 1, 0]\n",
      "[1, 1, 1, 1, 1, 1, -1, 1]\n",
      "[1, 0, 0, 0, 1, 0, 1, -1]\n"
     ]
    }
   ],
   "source": [
    "#example set\n",
    "for folder in os.listdir(main_path):\n",
    "    #check only folders, not files\n",
    "    if '.' not in folder:\n",
    "        set1=get_set(folder)\n",
    "        sets.append(set1)\n",
    "        break \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize 3D models and pointCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize some models and point clouds\n",
    "for set in sets:\n",
    "    for path in set[\"models\"]:\n",
    "        mesh = o3d.io.read_triangle_mesh(path)\n",
    "        pointcloud = mesh.sample_points_poisson_disk(1000)\n",
    "\n",
    "        # you can plot and check\n",
    "        #o3d.visualization.draw_geometries([mesh])\n",
    "        #o3d.visualization.draw_geometries([pointcloud])\n",
    "        \n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Used in class ModelNet40\\ndef download_modelnet40():\\n    BASE_DIR = os.path.dirname(os.path.abspath(\"\"))\\n    DATA_DIR = os.path.join(BASE_DIR, \\'data\\')\\n    if not os.path.exists(DATA_DIR):\\n        os.mkdir(DATA_DIR)\\n    if not os.path.exists(os.path.join(DATA_DIR, \\'modelnet40_ply_hdf5_2048\\')):\\n        print(\"dataset not available, download..\")\\n        www = \\'https://shapenet.cs.stanford.edu/media/modelnet40_ply_hdf5_2048.zip\\'\\n        zipfile = os.path.basename(www)\\n        os.system(\\'wget %s --no-check-certificate; unzip %s\\' % (www, zipfile))\\n        os.system(\\'mv %s %s\\' % (\\'modelnet40_ply_hdf5_2048\\', DATA_DIR))\\n        os.system(\\'rm %s\\' % (zipfile))\\n\\n#used in class ModelNet40\\ndef load_data_cls(partition):\\n    download_modelnet40()\\n    BASE_DIR = os.path.dirname(os.path.abspath(\"\"))\\n    DATA_DIR = os.path.join(BASE_DIR, \\'data\\')\\n    all_data = []\\n    all_label = []\\n    for h5_name in glob.glob(os.path.join(DATA_DIR, \\'modelnet40_ply_hdf5_2048\\', \\'*%s*.h5\\'%partition)):\\n        f = h5py.File(h5_name, \\'r+\\')\\n        data = f[\\'data\\'][:].astype(\\'float32\\')\\n        label = f[\\'label\\'][:].astype(\\'int64\\')\\n        f.close()\\n        all_data.append(data)\\n        all_label.append(label)\\n    all_data = np.concatenate(all_data, axis=0)\\n    all_label = np.concatenate(all_label, axis=0)\\n    return all_data, all_label\\n\\n#used in class ModelNet40\\ndef translate_pointcloud(pointcloud):\\n    xyz1 = np.random.uniform(low=2./3., high=3./2., size=[3])\\n    xyz2 = np.random.uniform(low=-0.2, high=0.2, size=[3])\\n       \\n    translated_pointcloud = np.add(np.multiply(pointcloud, xyz1), xyz2).astype(\\'float32\\')\\n    return translated_pointcloud\\n\\n#used in main\\nclass ModelNet40(Dataset):\\n    def __init__(self, num_points, partition=\\'train\\'):\\n        self.data, self.label = load_data_cls(partition)\\n        self.num_points = num_points\\n        self.partition = partition        \\n\\n    def __getitem__(self, item):\\n        pointcloud = self.data[item][:self.num_points]\\n        label = self.label[item]\\n        if self.partition == \\'train\\':\\n            pointcloud = translate_pointcloud(pointcloud)\\n            np.random.shuffle(pointcloud)\\n        return pointcloud, label\\n\\n    def __len__(self):\\n        return self.data.shape[0]\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "#Used in class ModelNet40\n",
    "def download_modelnet40():\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(\"\"))\n",
    "    DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        os.mkdir(DATA_DIR)\n",
    "    if not os.path.exists(os.path.join(DATA_DIR, 'modelnet40_ply_hdf5_2048')):\n",
    "        print(\"dataset not available, download..\")\n",
    "        www = 'https://shapenet.cs.stanford.edu/media/modelnet40_ply_hdf5_2048.zip'\n",
    "        zipfile = os.path.basename(www)\n",
    "        os.system('wget %s --no-check-certificate; unzip %s' % (www, zipfile))\n",
    "        os.system('mv %s %s' % ('modelnet40_ply_hdf5_2048', DATA_DIR))\n",
    "        os.system('rm %s' % (zipfile))\n",
    "\n",
    "#used in class ModelNet40\n",
    "def load_data_cls(partition):\n",
    "    download_modelnet40()\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(\"\"))\n",
    "    DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "    all_data = []\n",
    "    all_label = []\n",
    "    for h5_name in glob.glob(os.path.join(DATA_DIR, 'modelnet40_ply_hdf5_2048', '*%s*.h5'%partition)):\n",
    "        f = h5py.File(h5_name, 'r+')\n",
    "        data = f['data'][:].astype('float32')\n",
    "        label = f['label'][:].astype('int64')\n",
    "        f.close()\n",
    "        all_data.append(data)\n",
    "        all_label.append(label)\n",
    "    all_data = np.concatenate(all_data, axis=0)\n",
    "    all_label = np.concatenate(all_label, axis=0)\n",
    "    return all_data, all_label\n",
    "\n",
    "#used in class ModelNet40\n",
    "def translate_pointcloud(pointcloud):\n",
    "    xyz1 = np.random.uniform(low=2./3., high=3./2., size=[3])\n",
    "    xyz2 = np.random.uniform(low=-0.2, high=0.2, size=[3])\n",
    "       \n",
    "    translated_pointcloud = np.add(np.multiply(pointcloud, xyz1), xyz2).astype('float32')\n",
    "    return translated_pointcloud\n",
    "\n",
    "#used in main\n",
    "class ModelNet40(Dataset):\n",
    "    def __init__(self, num_points, partition='train'):\n",
    "        self.data, self.label = load_data_cls(partition)\n",
    "        self.num_points = num_points\n",
    "        self.partition = partition        \n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        pointcloud = self.data[item][:self.num_points]\n",
    "        label = self.label[item]\n",
    "        if self.partition == 'train':\n",
    "            pointcloud = translate_pointcloud(pointcloud)\n",
    "            np.random.shuffle(pointcloud)\n",
    "        return pointcloud, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here i save all the sets of fragments\n",
    "sets=[]\n",
    "\n",
    "#root folder\n",
    "main_path=\"produzione_29112021\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n"
     ]
    }
   ],
   "source": [
    "def create_pairs(num):\n",
    "    lista = []\n",
    "    for i in range(num):\n",
    "        for j in range(i+1, num):\n",
    "            lista.append((i, j))\n",
    "\n",
    "    return lista\n",
    "\n",
    "print(create_pairs(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(main_path,num_points):\n",
    "    n_folder=4\n",
    "    all_data=[]\n",
    "    all_labels=[]\n",
    "    sets=[]\n",
    "    for folder in os.listdir(main_path)[:n_folder]:\n",
    "        #check only folders, not files\n",
    "        if '.' in folder:\n",
    "            n_folder+=1\n",
    "        elif '.' not in folder:\n",
    "            fragment_set=get_set(folder,verbose=False)  #set of fragments of one model\n",
    "            sets.append(fragment_set)\n",
    "            set_pointcloud=[]\n",
    "            for path in fragment_set[\"models\"]:\n",
    "                mesh = o3d.io.read_triangle_mesh(path)\n",
    "                pointcloud = mesh.sample_points_poisson_disk(num_points)\n",
    "                set_pointcloud.append(pointcloud)\n",
    "\n",
    "            #get indeces of pairs\n",
    "            pairs=create_pairs(len(fragment_set[\"models\"]))\n",
    "            \n",
    "            #data=[]\n",
    "            #labels=[]\n",
    "            for pair in pairs:\n",
    "                idx1,idx2=pair\n",
    "                #data.append([set_pointcloud[idx1],set_pointcloud[idx2]])\n",
    "                #data.append([idx1,idx2])\n",
    "                label=fragment_set[\"grid\"][idx1][idx2]\n",
    "                #labels.append(label)\n",
    "                pair=[ np.asarray(set_pointcloud[idx1].points) ,  np.asarray(set_pointcloud[idx2].points)   ]\n",
    "                all_data.append(pair)\n",
    "                all_labels.append(label)\n",
    "\n",
    "                #np.asarray(pcd_load.points)\n",
    "\n",
    "    \n",
    "    return np.array(all_data),np.array(all_labels)\n",
    "                \n",
    "\n",
    "#used in main\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,main_path, num_points, partition='train'):\n",
    "        self.data, self.label = load_dataset(main_path,num_points)\n",
    "        self.num_points = num_points\n",
    "        self.partition = partition        \n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        #pointcloud1 = self.data[item][0]\n",
    "        #pointcloud2 = self.data[item][1]\n",
    "        pointclouds = self.data[item]\n",
    "        label = self.label[item]\n",
    "        #if self.partition == 'train':\n",
    "            #pointclouds = translate_pointcloud(pointcloud)\n",
    "            #np.random.shuffle(pointcloud)\n",
    "        return pointclouds, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_loader = DataLoader(MyDataset(partition='train',num_points=50,main_path=main_path), num_workers=0,\\n                              batch_size=5, shuffle=True, drop_last=True)\\nprint(len(train_loader))\\nfor data, label in train_loader:\\n    print(data.shape)\\n    print(label.shape)\\n\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example dataloader\n",
    "'''\n",
    "train_loader = DataLoader(MyDataset(partition='train',num_points=50,main_path=main_path), num_workers=0,\n",
    "                              batch_size=5, shuffle=True, drop_last=True)\n",
    "print(len(train_loader))\n",
    "for data, label in train_loader:\n",
    "    print(data.shape)\n",
    "    print(label.shape)\n",
    "\n",
    "'''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cal_loss(pred, gold, smoothing=False):\n",
    "    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
    "\n",
    "    gold = gold.contiguous().view(-1)\n",
    "\n",
    "    if smoothing:\n",
    "        eps = 0.2\n",
    "        n_class = pred.size(1)\n",
    "\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n",
    "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "        log_prb = F.log_softmax(pred, dim=1)\n",
    "\n",
    "        loss = -(one_hot * log_prb).sum(dim=1).mean()\n",
    "    else:\n",
    "        loss = F.binary_cross_entropy(pred, gold, reduction='mean')\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "class IOStream():\n",
    "    def __init__(self, path):\n",
    "        self.f = open(path, 'a')\n",
    "\n",
    "    def cprint(self, text):\n",
    "        print(text)\n",
    "        self.f.write(text+'\\n')\n",
    "        self.f.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def knn(x, k):\n",
    "    inner = -2*torch.matmul(x.transpose(2, 1), x)\n",
    "    xx = torch.sum(x**2, dim=1, keepdim=True)\n",
    "    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n",
    " \n",
    "    idx = pairwise_distance.topk(k=k, dim=-1)[1]   # (batch_size, num_points, k)\n",
    "    return idx\n",
    "\n",
    "#aggiunto aprametro device in modo da usare cpu se non gpu\n",
    "def get_graph_feature(x, k=20,device=\"cpu\", idx=None, dim9=False):\n",
    "    batch_size = x.size(0)\n",
    "    num_points = x.size(2)\n",
    "    #print(x.shape)\n",
    "    x = x.view(batch_size, -1, num_points)\n",
    "    #print(x.shape)\n",
    "\n",
    "    if idx is None:\n",
    "        if dim9 == False:\n",
    "            idx = knn(x, k=k)   # (batch_size, num_points, k)\n",
    "        else:\n",
    "            idx = knn(x[:, 6:], k=k)\n",
    "    device = torch.device(device)\n",
    "\n",
    "    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1)*num_points\n",
    "\n",
    "    idx = idx + idx_base\n",
    "\n",
    "    idx = idx.view(-1)\n",
    " \n",
    "    _, num_dims, _ = x.size()\n",
    "\n",
    "    x = x.transpose(2, 1).contiguous()   # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n",
    "    feature = x.view(batch_size*num_points, -1)[idx, :]\n",
    "    feature = feature.view(batch_size, num_points, k, num_dims) \n",
    "    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n",
    "    \n",
    "    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n",
    "  \n",
    "    return feature      # (batch_size, 2*num_dims, num_points, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PointNet(nn.Module):\n",
    "    def __init__(self, args, output_channels=40):\n",
    "        super(PointNet, self).__init__()\n",
    "        self.args = args\n",
    "        self.conv1 = nn.Conv1d(3, 64, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n",
    "        self.conv3 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n",
    "        self.conv4 = nn.Conv1d(64, 128, kernel_size=1, bias=False)\n",
    "        self.conv5 = nn.Conv1d(128, args[\"emb_dims\"], kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.bn5 = nn.BatchNorm1d(args[\"emb_dims\"])\n",
    "        self.linear1 = nn.Linear(args[\"emb_dims\"], 512, bias=False)\n",
    "        self.bn6 = nn.BatchNorm1d(512)\n",
    "        self.dp1 = nn.Dropout()\n",
    "        self.linear2 = nn.Linear(512, output_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = F.adaptive_max_pool1d(x, 1).squeeze()\n",
    "        x = F.relu(self.bn6(self.linear1(x)))\n",
    "        x = self.dp1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DGCNN_cls(nn.Module):\n",
    "    def __init__(self, args, output_channels=1):\n",
    "        super(DGCNN_cls, self).__init__()\n",
    "        self.args = args\n",
    "        self.device=\"cuda\" if self.args[\"cuda\"] else \"cpu\"\n",
    "        self.k = args[\"k\"]\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.bn5 = nn.BatchNorm1d(args[\"emb_dims\"])\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn1,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn2,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 128, kernel_size=1, bias=False),\n",
    "                                   self.bn3,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(128*2, 256, kernel_size=1, bias=False),\n",
    "                                   self.bn4,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv5 = nn.Sequential(nn.Conv1d(512, args[\"emb_dims\"], kernel_size=1, bias=False),\n",
    "                                   self.bn5,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.linear1 = nn.Linear(args[\"emb_dims\"]*2, 512, bias=False)\n",
    "        self.bn6 = nn.BatchNorm1d(512)\n",
    "        self.dp1 = nn.Dropout(p=args[\"dropout\"])\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.bn7 = nn.BatchNorm1d(256)\n",
    "        self.dp2 = nn.Dropout(p=args[\"dropout\"])\n",
    "        self.linear3 = nn.Linear(256, 256)\n",
    "\n",
    "        self.linear_classification = nn.Linear(512, output_channels)\n",
    "\n",
    "\n",
    "    #layer classficatore\n",
    "    def step(self,x):\n",
    "        batch_size = x.size(0)\n",
    "        x = get_graph_feature(x, k=self.k,device=self.device)      # (batch_size, 3, num_points) -> (batch_size, 3*2, num_points, k)\n",
    "        x = self.conv1(x)                       # (batch_size, 3*2, num_points, k) -> (batch_size, 64, num_points, k)\n",
    "        x1 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n",
    "\n",
    "        x = get_graph_feature(x1, k=self.k,device=self.device)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n",
    "        x = self.conv2(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n",
    "        x2 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n",
    "\n",
    "        x = get_graph_feature(x2, k=self.k,device=self.device)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n",
    "        x = self.conv3(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 128, num_points, k)\n",
    "        x3 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 128, num_points, k) -> (batch_size, 128, num_points)\n",
    "\n",
    "        x = get_graph_feature(x3, k=self.k,device=self.device)     # (batch_size, 128, num_points) -> (batch_size, 128*2, num_points, k)\n",
    "        x = self.conv4(x)                       # (batch_size, 128*2, num_points, k) -> (batch_size, 256, num_points, k)\n",
    "        x4 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 256, num_points, k) -> (batch_size, 256, num_points)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4), dim=1)  # (batch_size, 64+64+128+256, num_points)\n",
    "\n",
    "        x = self.conv5(x)                       # (batch_size, 64+64+128+256, num_points) -> (batch_size, emb_dims, num_points)\n",
    "        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)           # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims)\n",
    "        x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)           # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims)\n",
    "        x = torch.cat((x1, x2), 1)              # (batch_size, emb_dims*2)\n",
    "\n",
    "        x = F.leaky_relu(self.bn6(self.linear1(x)), negative_slope=0.2) # (batch_size, emb_dims*2) -> (batch_size, 512)\n",
    "        x = self.dp1(x)\n",
    "        x = F.leaky_relu(self.bn7(self.linear2(x)), negative_slope=0.2) # (batch_size, 512) -> (batch_size, 256)\n",
    "        x = self.dp2(x)\n",
    "        x = self.linear3(x)                                             # (batch_size, 256) -> (batch_size, output_channels)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        #first pointcloud\n",
    "        first=x[:,0,:,:]\n",
    "        second=x[:,1,:,:]\n",
    "\n",
    "        ret1=self.step(first)\n",
    "        \n",
    "        ret2=self.step(second)\n",
    "\n",
    "        #print(ret1.shape)\n",
    "    \n",
    "        ret=torch.cat((ret1,ret2),dim=1)\n",
    "        #print(ret.shape)\n",
    "\n",
    "        x=self.linear_classification(ret)\n",
    "\n",
    "        \n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={\n",
    "        \"exp_name\": \"exp\",          # Name of the experiment\n",
    "        \"model\": \"dgcnn\",           # Model to use, [pointnet, dgcnn]\n",
    "        \"dataset\": \"modelnet40\",    # Dataset to use\n",
    "        \"batch_size\": 16,           # Size of batch\n",
    "        \"test_batch_size\": 16,      # Size of batch\n",
    "        \"epochs\": 250,              # number of episode to train\n",
    "        \"use_sgd\": True,            # Use SGD\n",
    "        \"lr\": 0.001,                # learning rate (default: 0.001, 0.1 if using sgd)\n",
    "        \"momentum\": 0.9,            # SGD momentum (default: 0.9)\n",
    "        \"scheduler\": \"cos\",         # Scheduler to use, [cos, step]\n",
    "        \"no_cuda\": False,           # enables CUDA training\n",
    "        \"seed\": 1,                  # random seed (default: 1)\n",
    "        \"eval\": False,              # evaluate the model\n",
    "        \"num_points\": 100,         # num of points to use\n",
    "        \"dropout\": 0.5,             # initial dropout rate\n",
    "        \"emb_dims\": 1024,           # Dimension of embeddings\n",
    "        \"k\": 20,                    # Num of nearest neighbors to use\n",
    "        \"model_path\": \"\",           # Pretrained model path\n",
    "    }\n",
    "args[\"cuda\"]=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_loader = DataLoader(MyDataset(partition=\\'train\\', num_points=args[\"num_points\"],main_path=main_path), num_workers=0,\\n                            batch_size=args[\"batch_size\"], shuffle=True, drop_last=True)\\ndevice = torch.device(\"cuda\" if args[\"cuda\"] else \"cpu\")\\n\\nmodel = DGCNN_cls(args).to(device)\\nmodel.train()\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[batch_size,2,3,num_points]\n",
    "#[batch_size,3,num_points]\n",
    "#x=[]\n",
    "#for elem in data:\n",
    "#    x.append[elem[0]]\n",
    "\n",
    "#data[:,0,:,:]\n",
    "'''train_loader = DataLoader(MyDataset(partition='train', num_points=args[\"num_points\"],main_path=main_path), num_workers=0,\n",
    "                            batch_size=args[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "device = torch.device(\"cuda\" if args[\"cuda\"] else \"cpu\")\n",
    "\n",
    "model = DGCNN_cls(args).to(device)\n",
    "model.train()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor data, label in train_loader:\\n    data, label = data.to(device), label.to(device).squeeze()\\n    print(label)\\n    data=data.float()\\n    label=label.float()\\n    data = data.permute(0,1,3,2)\\n    batch_size = data.size()[0]\\n    logits = model(data)\\n    logits=torch.squeeze(F.sigmoid(logits))\\n    print(logits)\\n    \\n    loss = cal_loss(logits, label)\\n    print(\"loss: \",loss)\\n    loss.backward()\\n    break\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for data, label in train_loader:\n",
    "    data, label = data.to(device), label.to(device).squeeze()\n",
    "    print(label)\n",
    "    data=data.float()\n",
    "    label=label.float()\n",
    "    data = data.permute(0,1,3,2)\n",
    "    batch_size = data.size()[0]\n",
    "    logits = model(data)\n",
    "    logits=torch.squeeze(F.sigmoid(logits))\n",
    "    print(logits)\n",
    "    \n",
    "    loss = cal_loss(logits, label)\n",
    "    print(\"loss: \",loss)\n",
    "    loss.backward()\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error to stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_():\n",
    "    if not os.path.exists('outputs'):\n",
    "        os.makedirs('outputs')\n",
    "    if not os.path.exists('outputs/'+args[\"exp_name\"]):\n",
    "        os.makedirs('outputs/'+args[\"exp_name\"])\n",
    "    if not os.path.exists('outputs/'+args[\"exp_name\"]+'/'+'models'):\n",
    "        os.makedirs('outputs/'+args[\"exp_name\"]+'/'+'models')\n",
    "    os.system('cp main_cls.py outputs'+'/'+args[\"exp_name\"]+'/'+'main_cls.py.backup')\n",
    "    os.system('cp model.py outputs' + '/' + args[\"exp_name\"] + '/' + 'model.py.backup')\n",
    "    os.system('cp util.py outputs' + '/' + args[\"exp_name\"] + '/' + 'util.py.backup')\n",
    "    os.system('cp data.py outputs' + '/' + args[\"exp_name\"] + '/' + 'data.py.backup')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(args, io):\n",
    "    # num_workers=0 to make it work on windows\n",
    "    train_loader = DataLoader(MyDataset(partition='train', num_points=args[\"num_points\"],main_path=main_path), num_workers=0,\n",
    "                              batch_size=args[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(MyDataset(partition='test', num_points=args[\"num_points\"],main_path=main_path), num_workers=0,\n",
    "                             batch_size=args[\"test_batch_size\"], shuffle=True, drop_last=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if args[\"cuda\"] else \"cpu\")\n",
    "\n",
    "    #Try to load models\n",
    "    if args[\"model\"] == 'pointnet':\n",
    "        model = PointNet(args).to(device)\n",
    "    elif args[\"model\"] == 'dgcnn':\n",
    "        model = DGCNN_cls(args).to(device)\n",
    "    else:\n",
    "        raise Exception(\"Not implemented\")\n",
    "\n",
    "    print(str(model))\n",
    "\n",
    "    model = nn.DataParallel(model)\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "\n",
    "    if args[\"use_sgd\"]:\n",
    "        print(\"Use SGD\")\n",
    "        opt = optim.SGD(model.parameters(), lr=args[\"lr\"]*100,\n",
    "                momentum=args[\"momentum\"], weight_decay=1e-4)\n",
    "    else:\n",
    "        print(\"Use Adam\")\n",
    "        opt = optim.Adam(model.parameters(), lr=args[\"lr\"], weight_decay=1e-4)\n",
    "\n",
    "    if args[\"scheduler\"] == 'cos':\n",
    "        scheduler = CosineAnnealingLR(opt, args[\"epochs\"], eta_min=1e-3)\n",
    "    elif args[\"scheduler\"] == 'step':\n",
    "        scheduler = StepLR(opt, step_size=20, gamma=0.7)\n",
    "    \n",
    "    criterion = cal_loss\n",
    "\n",
    "    best_test_acc = 0\n",
    "    print(args)\n",
    "    for epoch in range(args[\"epochs\"]):\n",
    "        ####################\n",
    "        # Train\n",
    "        ####################\n",
    "        train_loss = 0.0\n",
    "        count = 0.0\n",
    "        model.train()\n",
    "        train_pred = []\n",
    "        train_true = []\n",
    "        print(\"start epoch\")\n",
    "        for data, label in train_loader:\n",
    "            #print(\"get data\")\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            #print(label)\n",
    "            data=data.float()\n",
    "            label=label.float()\n",
    "            data = data.permute(0,1,3,2)\n",
    "            batch_size = data.size()[0]\n",
    "            opt.zero_grad()\n",
    "            logits = model(data)\n",
    "            logits=torch.squeeze(F.sigmoid(logits))\n",
    "            loss = criterion(logits, label)\n",
    "            \n",
    "           \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            #preds = logits.max(dim=1)[1] #not needed, we have only one value for each sample\n",
    "            preds=logits.int()\n",
    "            count += batch_size\n",
    "            train_loss += loss.item() * batch_size\n",
    "            train_true.append(label.cpu().numpy())\n",
    "            train_pred.append(preds.detach().cpu().numpy())\n",
    "        \n",
    "        if args[\"scheduler\"] == 'cos':\n",
    "            scheduler.step()\n",
    "        elif args[\"scheduler\"] == 'step':\n",
    "            if opt.param_groups[0]['lr'] > 1e-5:\n",
    "                scheduler.step()\n",
    "            if opt.param_groups[0]['lr'] < 1e-5:\n",
    "                for param_group in opt.param_groups:\n",
    "                    param_group['lr'] = 1e-5\n",
    "\n",
    "        \n",
    "        train_true = np.concatenate(train_true)\n",
    "        train_pred = np.concatenate(train_pred)\n",
    "        outstr = 'Train %d, loss: %.6f, train acc: %.6f, train avg acc: %.6f' % (epoch,\n",
    "                                                                                 train_loss*1.0/count,\n",
    "                                                                                 metrics.accuracy_score(\n",
    "                                                                                     train_true, train_pred),\n",
    "                                                                                 metrics.balanced_accuracy_score(\n",
    "                                                                                     train_true, train_pred))\n",
    "        io.cprint(outstr)\n",
    "\n",
    "        ####################\n",
    "        # Test\n",
    "        ####################\n",
    "        test_loss = 0.0\n",
    "        count = 0.0\n",
    "        model.eval()\n",
    "        test_pred = []\n",
    "        test_true = []\n",
    "        for data, label in test_loader:\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            data=data.float()\n",
    "            label=label.float()\n",
    "\n",
    "            data = data.permute(0,1,3,2)\n",
    "            batch_size = data.size()[0]\n",
    "            logits = model(data)\n",
    "            logits=torch.squeeze(F.sigmoid(logits))\n",
    "            loss = criterion(logits, label)\n",
    "            \n",
    "            #preds = logits.max(dim=1)[1] #not needed, we have only one value for each sample\n",
    "            preds=logits.int()\n",
    "\n",
    "            count += batch_size\n",
    "            test_loss += loss.item() * batch_size\n",
    "            test_true.append(label.cpu().numpy())\n",
    "            test_pred.append(preds.detach().cpu().numpy())\n",
    "        test_true = np.concatenate(test_true)\n",
    "        test_pred = np.concatenate(test_pred)\n",
    "        test_acc = metrics.accuracy_score(test_true, test_pred)\n",
    "        avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n",
    "        outstr = 'Test %d, loss: %.6f, test acc: %.6f, test avg acc: %.6f' % (epoch,\n",
    "                                                                              test_loss*1.0/count,\n",
    "                                                                              test_acc,\n",
    "                                                                              avg_per_class_acc)\n",
    "        io.cprint(outstr)\n",
    "        ''' \n",
    "        if test_acc >= best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            torch.save(model.state_dict(), 'outputs/%s/models/model.t7' % args[\"use_sgd\"])'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(args, io):\n",
    "    test_loader = DataLoader(ModelNet40(partition='test', num_points=args[\"num_points\"]),\n",
    "                             batch_size=args[\"test_batch_size\"], shuffle=True, drop_last=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if args[\"cuda\"] else \"cpu\")\n",
    "\n",
    "    #Try to load models\n",
    "    if args[\"mdoel\"] == 'pointnet':\n",
    "        model = PointNet(args).to(device)\n",
    "    elif args[\"mdoel\"] == 'dgcnn':\n",
    "        model = DGCNN_cls(args).to(device)\n",
    "    else:\n",
    "        raise Exception(\"Not implemented\")\n",
    "\n",
    "    model = nn.DataParallel(model)\n",
    "    model.load_state_dict(torch.load(args[\"model_path\"]))\n",
    "    model = model.eval()\n",
    "    test_acc = 0.0\n",
    "    count = 0.0\n",
    "    test_true = []\n",
    "    test_pred = []\n",
    "    for data, label in test_loader:\n",
    "\n",
    "        data, label = data.to(device), label.to(device).squeeze()\n",
    "        data = data.permute(0, 2, 1)\n",
    "        batch_size = data.size()[0]\n",
    "        logits = model(data)\n",
    "        #preds = logits.max(dim=1)[1]\n",
    "        preds=logits.int()\n",
    "        test_true.append(label.cpu().numpy())\n",
    "        test_pred.append(preds.detach().cpu().numpy())\n",
    "    test_true = np.concatenate(test_true)\n",
    "    test_pred = np.concatenate(test_pred)\n",
    "    test_acc = metrics.accuracy_score(test_true, test_pred)\n",
    "    avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n",
    "    outstr = 'Test :: test acc: %.6f, test avg acc: %.6f'%(test_acc, avg_per_class_acc)\n",
    "    io.cprint(outstr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exp_name': 'exp', 'model': 'dgcnn', 'dataset': 'modelnet40', 'batch_size': 16, 'test_batch_size': 16, 'epochs': 50, 'use_sgd': True, 'lr': 0.001, 'momentum': 0.9, 'scheduler': 'cos', 'no_cuda': False, 'seed': 1, 'eval': False, 'num_points': 100, 'dropout': 0.5, 'emb_dims': 1024, 'k': 20, 'model_path': ''}\n",
      "Using CPU\n",
      "DGCNN_cls(\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(6, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv5): Sequential(\n",
      "    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (linear1): Linear(in_features=2048, out_features=512, bias=False)\n",
      "  (bn6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dp1): Dropout(p=0.5, inplace=False)\n",
      "  (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (bn7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dp2): Dropout(p=0.5, inplace=False)\n",
      "  (linear3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (linear_classification): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "Let's use 0 GPUs!\n",
      "Use SGD\n",
      "{'exp_name': 'exp', 'model': 'dgcnn', 'dataset': 'modelnet40', 'batch_size': 16, 'test_batch_size': 16, 'epochs': 50, 'use_sgd': True, 'lr': 0.001, 'momentum': 0.9, 'scheduler': 'cos', 'no_cuda': False, 'seed': 1, 'eval': False, 'num_points': 100, 'dropout': 0.5, 'emb_dims': 1024, 'k': 20, 'model_path': '', 'cuda': False}\n",
      "start epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alessio\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0, loss: 0.746149, train acc: 0.333333, train avg acc: 0.500000\n",
      "Test 0, loss: 0.675970, test acc: 0.321429, test avg acc: 0.500000\n",
      "start epoch\n",
      "Train 1, loss: 0.790861, train acc: 0.291667, train avg acc: 0.500000\n",
      "Test 1, loss: 1.222614, test acc: 0.321429, test avg acc: 0.500000\n",
      "start epoch\n",
      "Train 2, loss: 0.797122, train acc: 0.312500, train avg acc: 0.500000\n",
      "Test 2, loss: 1.483345, test acc: 0.321429, test avg acc: 0.500000\n",
      "start epoch\n",
      "Train 3, loss: 1.643998, train acc: 0.354167, train avg acc: 0.500000\n",
      "Test 3, loss: 2.358472, test acc: 0.321429, test avg acc: 0.500000\n",
      "start epoch\n",
      "Train 4, loss: 2.687855, train acc: 0.312500, train avg acc: 0.500000\n",
      "Test 4, loss: 1.542577, test acc: 0.321429, test avg acc: 0.500000\n",
      "start epoch\n",
      "Train 5, loss: 1.751346, train acc: 0.354167, train avg acc: 0.500000\n",
      "Test 5, loss: 0.830006, test acc: 0.321429, test avg acc: 0.500000\n",
      "start epoch\n",
      "Train 6, loss: 1.592050, train acc: 0.312500, train avg acc: 0.500000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-1bf91759ac2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"eval\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-b9fec7d90cf7>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args, io)\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\EAI\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-70efb1139e10>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0msecond\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mret1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mret2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msecond\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-70efb1139e10>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_graph_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;31m# (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m    \u001b[1;31m# (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_graph_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    args={\n",
    "        \"exp_name\": \"exp\",          # Name of the experiment\n",
    "        \"model\": \"dgcnn\",           # Model to use, [pointnet, dgcnn]\n",
    "        \"dataset\": \"modelnet40\",    # Dataset to use\n",
    "        \"batch_size\": 16,           # Size of batch\n",
    "        \"test_batch_size\": 16,      # Size of batch\n",
    "        \"epochs\": 50,              # number of episode to train\n",
    "        \"use_sgd\": True,            # Use SGD\n",
    "        \"lr\": 0.001,                # learning rate (default: 0.001, 0.1 if using sgd)\n",
    "        \"momentum\": 0.9,            # SGD momentum (default: 0.9)\n",
    "        \"scheduler\": \"cos\",         # Scheduler to use, [cos, step]\n",
    "        \"no_cuda\": False,           # enables CUDA training\n",
    "        \"seed\": 1,                  # random seed (default: 1)\n",
    "        \"eval\": False,              # evaluate the model\n",
    "        \"num_points\": 100,         # num of points to use\n",
    "        \"dropout\": 0.5,             # initial dropout rate\n",
    "        \"emb_dims\": 1024,           # Dimension of embeddings\n",
    "        \"k\": 20,                    # Num of nearest neighbors to use\n",
    "        \"model_path\": \"\",           # Pretrained model path\n",
    "    }\n",
    "\n",
    "    _init_()\n",
    "\n",
    "    io = IOStream('outputs/' + args[\"exp_name\"] + '/run.log')\n",
    "    io.cprint(str(args))\n",
    "\n",
    "    args[\"cuda\"] = not args[\"no_cuda\"] and torch.cuda.is_available()\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "    if args[\"cuda\"]:\n",
    "        io.cprint(\n",
    "            'Using GPU : ' + str(torch.cuda.current_device()) + ' from ' + str(torch.cuda.device_count()) + ' devices')\n",
    "        torch.cuda.manual_seed(args[\"seed\"])\n",
    "    else:\n",
    "        io.cprint('Using CPU')\n",
    "\n",
    "    if not args[\"eval\"]:\n",
    "        train(args, io)\n",
    "    else:\n",
    "        test(args, io)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "691f1b564724e08df0ae28181cdfea598ff9582372b176e10f27b62d8e6609e2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
